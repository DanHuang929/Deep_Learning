{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e8140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HMILAB\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2cc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, \\\n",
    "Conv2D, Conv2DTranspose, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6feb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1926ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a61f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63f827",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "Since dealing with raw string is inefficient, we have done some data preprocessing for you:\n",
    "\n",
    "* Delete text over **MAX_SEQ_LENGTH (20)**.\n",
    "* Delete all puntuation in the texts.\n",
    "* Encode each vocabulary in **dictionary/vocab.npy**.\n",
    "* Represent texts by a sequence of integer IDs.\n",
    "* Replace rare words by **\\<RARE>** token to reduce vocabulary size for more efficient training.\n",
    "* Add padding as **\\<PAD>** to each text to make sure all of them have equal length to **MAX_SEQ_LENGTH (20)**.\n",
    "It is worth knowing that there is no necessary to append **\\<ST>** and **\\<ED>** to each text because we don't need to generate any sequence in this task.\n",
    "\n",
    "To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:\n",
    "\n",
    "* dictionary/word2Id.npy is a numpy array mapping word to id.\n",
    "* dictionary/id2Word.npy is a numpy array mapping id back to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5626eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e02e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157288fb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For training, the following files are in dataset folder:\n",
    "\n",
    "* **./dataset/text2ImgData.pkl** is a pandas dataframe with attribute 'Captions' and 'ImagePath'.\n",
    "    * 'Captions' : A list of text id list contain 1 to 10 captions.\n",
    "    * 'ImagePath': Image path that store paired image.\n",
    "* **./102flowers/** is the directory containing all training images.\n",
    "* **./dataset/testData.pkl** is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fdd93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80de249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf6a2d",
   "metadata": {},
   "source": [
    "## Create Dataset by Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 0.0002,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 10000,\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e1fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    \n",
    "    if(random.random() < 0.5):\n",
    "        img = tf.image.stateless_random_crop(img, (48, 48, 3), seed = (0,1))\n",
    "        img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8e877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', hparas[\"BATCH_SIZE\"], training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984aaa2",
   "metadata": {},
   "source": [
    "# Text Encoder\n",
    "\n",
    "在助教提供的template中是使用GNU，而我們則是選擇採用BERT這個pretrained model來當作我們的text encoder，用來取得embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f761371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2sentence(id2word_dict, id_sent):\n",
    "    batch_sentence = []\n",
    "    for j in range(id_sent.shape[0]):\n",
    "        sent=\"\"\n",
    "        for i in range(len(id_sent[j])):\n",
    "            if id2word_dict[str(id_sent[j][i].numpy())]==\"<PAD>\":\n",
    "                continue\n",
    "            sent=sent+\" \"+(id2word_dict[str(id_sent[j][i].numpy())])\n",
    "        \n",
    "        batch_sentence.append(sent)\n",
    "    return batch_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56631dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3de1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(caption):\n",
    "    try:\n",
    "        string_list = id2sentence(id2word_dict, caption)\n",
    "        bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "        bert_outputs = bert_model(bert_inputs)\n",
    "        caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    except(ValueError):\n",
    "        print(string_list)\n",
    "    return tf.convert_to_tensor(caption_embedding.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (image, caption) in enumerate(dataset):\n",
    "#     image=np.array(image)\n",
    "#     plt.imshow(image[0])\n",
    "#     plt.show()\n",
    "#     text_embed = text_encoder(caption)\n",
    "#     print(text_embed[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a2acf",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "我們的架構如下：\n",
    "1. 先將text embedding flatten並傳入一層dense layer，然後和noise concate起來後再傳入一層dense layer，normalize並reshape後產生第一個output x0。\n",
    "2. 之後，將x0傳入三層convolutional layer，中間穿插batch normalization，產生第二個output x1。\n",
    "3. 接著，將x0、x1相加，經過一層convolutional transpose layer並normalizaiton，產生第三個output x2。\n",
    "4. 把x2傳入三層convolutional layer，中間穿插batch normalization，產生第四個output x。\n",
    "5. 把x、x2相加，經過兩層convolutional transpose layer並穿插normalizaiton，產生最終的logits。\n",
    "6. 最後用tanh產生最終output。\n",
    "\n",
    "此外，在上述架構中，有用到activation function的部份我們全部是採用relu，並將relu的alpha值設為0.2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4214c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10000\n",
    "LEAKY_ALPHA = 0.2\n",
    "def my_leaky_relu(tensor):\n",
    "    return tf.nn.leaky_relu(tensor, alpha=LEAKY_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d81f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = Flatten()\n",
    "#         self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "#         self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d1 = Dense(self.hparas['DENSE_DIM'], activation=my_leaky_relu)\n",
    "        self.d2 = Dense(128*8*4*4)\n",
    "        self.BN0 = BatchNormalization()\n",
    "        \n",
    "        self.conv1 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN1 = BatchNormalization()\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(\n",
    "            filters=128*8,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        \n",
    "        self.BN3 = BatchNormalization()\n",
    "        \n",
    "        self.conv4_T = Conv2DTranspose(\n",
    "            filters=128*4,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN4 = BatchNormalization()\n",
    "        \n",
    "        self.conv5 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN5 = BatchNormalization()\n",
    "        \n",
    "        self.conv6 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN6 = BatchNormalization()\n",
    "        \n",
    "        \n",
    "        self.conv7 = Conv2D(\n",
    "            filters=128*4,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN7 = BatchNormalization()\n",
    "        \n",
    "        self.conv8_T = Conv2DTranspose(\n",
    "            filters=128*2,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN8 = BatchNormalization()\n",
    "        \n",
    "        self.conv9_T = Conv2DTranspose(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN9 = BatchNormalization()\n",
    "    \n",
    "        self.out = Conv2DTranspose(\n",
    "            filters=3,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "#             activation=tf.tanh\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "\n",
    "        text = self.flatten(text)\n",
    "        x0 = self.d1(text)\n",
    "        x0 = tf.concat([noise_z, x0], axis=1)\n",
    "        x0 = self.d2(x0)\n",
    "        x0 = self.BN0(x0)\n",
    "        x0 = tf.reshape(x0, shape=[-1, 4, 4, 128*8])\n",
    "        \n",
    "        x1 = self.conv1(x0)\n",
    "        x1 = self.BN1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.BN2(x1)\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = self.BN3(x1)\n",
    "        \n",
    "        x2 = tf.add(x0, x1)\n",
    "        x2 = self.conv4_T(x2)\n",
    "        x2 = self.BN4(x2)\n",
    "        x = self.conv5(x2)\n",
    "        x = self.BN5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.BN6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.BN7(x)\n",
    "        \n",
    "        x3 = tf.add(x2, x)\n",
    "        x3 = self.conv8_T(x3)\n",
    "        x3 = self.BN8(x3)\n",
    "        x3 = self.conv9_T(x3)\n",
    "        x3 = self.BN9(x3)\n",
    "        \n",
    "        logits = self.out(x3)\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e00cea",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "\n",
    "我們的架構如下:\n",
    "\n",
    "1. 先單獨處理圖片:\n",
    "    * 把圖片經過一個convolutional layer後，再經過三次convolutional layer和batch normalization的穿插，得到第一個output x0。\n",
    "    * 將x0再經過三次convolutional layer和batch normalization的穿插，得到output x。\n",
    "    * 把x和x0相加，得到output x1。\n",
    "2. 加入text:\n",
    "    * 將text經過預處理(tile、expand_dim)，然後和x1 concate起來。\n",
    "    * 經過一層convolutional layer和一層normalizaiton，產生最終的logits。\n",
    "3. 最後用tanh產生最終output。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b665e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        \n",
    "        self.conv1 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.BN2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.BN3 = BatchNormalization()\n",
    "        \n",
    "        self.conv4 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=None\n",
    "        )\n",
    "        self.BN4 = BatchNormalization()\n",
    "        \n",
    "        self.conv5 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN5 = BatchNormalization()\n",
    "        \n",
    "        self.conv6 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN6 = BatchNormalization()\n",
    "        \n",
    "        self.conv7 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN7 = BatchNormalization()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #text\n",
    "        \n",
    "        self.d1 = Dense(self.hparas['DENSE_DIM'], activation=my_leaky_relu)\n",
    "        \n",
    "        self.conv8 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN8 = BatchNormalization()\n",
    "        \n",
    "        self.out = Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[4, 4],\n",
    "            padding=\"valid\"\n",
    "        )\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        \n",
    "        x0 = self.conv1(img)\n",
    "        x0 = self.conv2(x0)\n",
    "        x0 = self.BN2(x0)\n",
    "        x0 = self.conv3(x0)\n",
    "        x0 = self.BN3(x0)\n",
    "        x0 = self.conv4(x0)\n",
    "        x0 = self.BN4(x0)\n",
    "        \n",
    "        x = self.conv5(x0)\n",
    "        x = self.BN5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.BN6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.BN7(x)\n",
    "        \n",
    "        x1 = tf.add(x0, x)\n",
    "        \n",
    "        #text\n",
    "        x2 = self.d1(text)\n",
    "        x2 = tf.expand_dims(x2, axis=1)\n",
    "        x2 = tf.expand_dims(x2, axis=1)\n",
    "        \n",
    "        x2 = tf.tile(x2, multiples=[1, 4, 4, 1])\n",
    "        x3 = tf.concat(values=[x1, x2], axis=3)\n",
    "        x3 = self.conv8(x3)\n",
    "        x3 = self.BN8(x3)\n",
    "        \n",
    "        logits = self.out(x3)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b02a8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "380606e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (image, caption) in enumerate(dataset):   \n",
    "    text_embed = text_encoder(caption)   \n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    fake_logits, fake_output = discriminator(fake_image, text_embed)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4763f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(\"models/W_generator_bert.h5\")\n",
    "discriminator.load_weights(\"models/W_discriminator_bert.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4f761",
   "metadata": {},
   "source": [
    "## Loss Function and Optimization\n",
    "\n",
    "loss的計算方式，我們並不像template一樣單純將condition加入GAN，而是有使用類似improved WGAN的方式來計算。\n",
    "和improved WGAN相同的，我們有用到gradient penalty來限制interpolate的gradient不能大於1。\n",
    "以下是我們loss計算方式的特點:\n",
    "\n",
    "1. 和improved WGAN一樣，在fake image和real image之間以隨機比例取得interpolate。\n",
    "2. 除了(real_image, text)、(fake_image, text)的pair外，多加了第3個pair: (interpolate, text)\n",
    "3. 用第三個pair丟入discriminator，產生的logits和interpolate計算gradient作為限制的penalty。\n",
    "4. 最後，和improved WGAN一樣，在loss的計算中加上penalty。\n",
    "5. optimizer的部分同樣使用Adam，generator部分的learning rate設為1e-4，discriminator部分的learning rate設為1e-4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8960d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1=hparas['BETA_1'])\n",
    "optimizer_d = tf.keras.optimizers.Adam(learning_rate = 3e-4, beta_1=hparas['BETA_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2898ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "@tf.function\n",
    "def WGTrain(real_image, caption, noise_decay):\n",
    "    #c1: true image\n",
    "#     z = tf.random.normal(BZ)\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    with tf.GradientTape() as tpg, tf.GradientTape() as inter_tpg:\n",
    "        text_embed = text_encoder(caption)\n",
    "        \n",
    "        #c0: fake image\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        alpha = tf.random.uniform([hparas['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = alpha * fake_image + (1.0 - alpha) * real_image\n",
    "        \n",
    "        fake_image = fake_image + noise_decay * tf.random.normal(fake_image.shape)\n",
    "        real_image = real_image + noise_decay * tf.random.normal(real_image.shape)\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        \n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "        mismatch_logits, mismatch_output = discriminator(interpolates, text_embed)\n",
    "\n",
    "        gradient_penalty = inter_tpg.gradient(mismatch_logits,interpolates)\n",
    "        gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "        loss = fake_logits - real_logits + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "        ld = tf.reduce_mean(loss)\n",
    "        lg = - tf.reduce_mean(fake_logits)\n",
    "\n",
    "    gradient_g = tpg.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    optimizer_g.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "    \n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def WDTrain(real_image, caption, noise_decay):\n",
    "    #c1: true image\n",
    "#     z = tf.random.normal(BZ)\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tpd, tf.GradientTape() as inter_tpg:\n",
    "        text_embed = text_encoder(caption)\n",
    "        \n",
    "        \n",
    "        #c0: fake image\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        alpha = tf.random.uniform([hparas['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = alpha * fake_image + (1.0 - alpha) * real_image\n",
    "        \n",
    "        fake_image = fake_image + noise_decay * tf.random.normal(fake_image.shape)\n",
    "        real_image = real_image + noise_decay * tf.random.normal(real_image.shape)\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        \n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "        mismatch_logits, mismatch_output = discriminator(interpolates, text_embed)\n",
    "\n",
    "        gradient_penalty = inter_tpg.gradient(mismatch_logits,interpolates)\n",
    "        gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "        loss = fake_logits - real_logits + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "        ld = tf.reduce_mean(loss)\n",
    "        lg = - tf.reduce_mean(fake_logits)\n",
    "\n",
    "\n",
    "    gradient_d = tpd.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    optimizer_d.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "   \n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a25a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise):\n",
    "    text_embed = text_encoder(caption)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766cfb5",
   "metadata": {},
   "source": [
    "### Visualiztion\n",
    "During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c153399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "098be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21347d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"flower with white long white petals and very long purple stamen.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "print(len(sample_sentence))\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6674c7e",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "之前的lab在練習GAN時，是每5個discriminator的train之後接上1個generator的train，但這次助教的template中卻沒有這種比例調整。\n",
    "\n",
    "我們這次採用的則是每2個discriminator的train之後接上1個generator的train。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51bc3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "WTrain = (\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "#     WDTrain,\n",
    "#     WDTrain,\n",
    "#     WDTrain,\n",
    "    WGTrain\n",
    ")\n",
    "\n",
    "WCritic = len(WTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7176ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    ctr = 0\n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "#         if epoch < 200:\n",
    "#             noise_decay = 1.0 / float(epoch+1)\n",
    "#         else:\n",
    "        noise_decay = 0.0\n",
    "        \n",
    "        for step, (image, caption) in enumerate(dataset):\n",
    "            g_loss, d_loss = WTrain[ctr](image, caption, noise_decay)\n",
    "            ctr += 1\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            if ctr == WCritic : \n",
    "                ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        generator.save_weights(\"models/W_generator_bert.h5\")\n",
    "        discriminator.save_weights(\"models/W_discriminator_bert.h5\")\n",
    "        \n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed)\n",
    "            save_images(fake_image, [ni, ni], 'samples/bert_demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d6aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 119.2205, disc_loss: -3.5303\n",
      "Time for epoch 1 is 49.9389 sec\n",
      "Epoch 2, gen_loss: 90.1806, disc_loss: -2.0542\n",
      "Time for epoch 2 is 57.7671 sec\n",
      "Epoch 3, gen_loss: 137.0713, disc_loss: -1.5452\n",
      "Time for epoch 3 is 48.3446 sec\n",
      "Epoch 4, gen_loss: 105.6682, disc_loss: -1.7707\n",
      "Time for epoch 4 is 49.7348 sec\n",
      "Epoch 5, gen_loss: 121.8130, disc_loss: -2.1540\n",
      "Time for epoch 5 is 47.7680 sec\n",
      "Epoch 6, gen_loss: 143.5983, disc_loss: -2.0513\n",
      "Time for epoch 6 is 48.3780 sec\n",
      "Epoch 7, gen_loss: 156.4766, disc_loss: -1.9120\n",
      "Time for epoch 7 is 48.2201 sec\n",
      "Epoch 8, gen_loss: 131.8680, disc_loss: -1.8125\n",
      "Time for epoch 8 is 48.0337 sec\n",
      "Epoch 9, gen_loss: 123.0075, disc_loss: -1.9908\n",
      "Time for epoch 9 is 48.3302 sec\n",
      "Epoch 10, gen_loss: 115.4368, disc_loss: -2.1229\n",
      "Time for epoch 10 is 48.6423 sec\n",
      "Epoch 11, gen_loss: 116.6673, disc_loss: -1.8037\n",
      "Time for epoch 11 is 48.4068 sec\n",
      "Epoch 12, gen_loss: 142.3567, disc_loss: -1.8692\n",
      "Time for epoch 12 is 48.7815 sec\n",
      "Epoch 13, gen_loss: 154.0032, disc_loss: -2.1536\n",
      "Time for epoch 13 is 48.0472 sec\n",
      "Epoch 14, gen_loss: 160.1536, disc_loss: -1.8737\n",
      "Time for epoch 14 is 48.4547 sec\n",
      "Epoch 15, gen_loss: 195.7664, disc_loss: -1.9779\n",
      "Time for epoch 15 is 48.4706 sec\n",
      "Epoch 16, gen_loss: 153.7786, disc_loss: -2.2332\n",
      "Time for epoch 16 is 48.4548 sec\n",
      "Epoch 17, gen_loss: 148.5178, disc_loss: -2.0463\n",
      "Time for epoch 17 is 48.2976 sec\n",
      "Epoch 18, gen_loss: 151.3134, disc_loss: -1.6149\n",
      "Time for epoch 18 is 48.7306 sec\n",
      "Epoch 19, gen_loss: 147.1009, disc_loss: -1.6727\n",
      "Time for epoch 19 is 47.6993 sec\n",
      "Epoch 20, gen_loss: 152.4223, disc_loss: -2.0884\n",
      "Time for epoch 20 is 48.5956 sec\n",
      "Epoch 21, gen_loss: 157.9286, disc_loss: -1.9723\n",
      "Time for epoch 21 is 48.2034 sec\n",
      "Epoch 22, gen_loss: 167.8491, disc_loss: -1.8498\n",
      "Time for epoch 22 is 47.9541 sec\n",
      "Epoch 23, gen_loss: 153.2590, disc_loss: -2.3196\n",
      "Time for epoch 23 is 48.4384 sec\n",
      "Epoch 24, gen_loss: 160.9101, disc_loss: -1.9841\n",
      "Time for epoch 24 is 48.2987 sec\n",
      "Epoch 25, gen_loss: 152.6261, disc_loss: -1.9459\n",
      "Time for epoch 25 is 48.5165 sec\n",
      "Epoch 26, gen_loss: 157.2374, disc_loss: -1.6293\n",
      "Time for epoch 26 is 48.3300 sec\n",
      "Epoch 27, gen_loss: 204.7604, disc_loss: -2.0103\n",
      "Time for epoch 27 is 48.3937 sec\n",
      "Epoch 28, gen_loss: 168.7047, disc_loss: -2.4133\n",
      "Time for epoch 28 is 48.3756 sec\n",
      "Epoch 29, gen_loss: 214.3082, disc_loss: -2.1682\n",
      "Time for epoch 29 is 48.5957 sec\n",
      "Epoch 30, gen_loss: 188.3131, disc_loss: -2.0452\n",
      "Time for epoch 30 is 48.7664 sec\n",
      "Epoch 31, gen_loss: 185.3760, disc_loss: -2.1661\n",
      "Time for epoch 31 is 48.4688 sec\n",
      "Epoch 32, gen_loss: 201.4276, disc_loss: -1.9406\n",
      "Time for epoch 32 is 48.1857 sec\n",
      "Epoch 33, gen_loss: 158.2030, disc_loss: -1.9907\n",
      "Time for epoch 33 is 48.8143 sec\n",
      "Epoch 34, gen_loss: 161.3638, disc_loss: -2.0060\n",
      "Time for epoch 34 is 48.2562 sec\n",
      "Epoch 35, gen_loss: 185.8693, disc_loss: -1.9751\n",
      "Time for epoch 35 is 48.5896 sec\n",
      "Epoch 36, gen_loss: 165.5727, disc_loss: -2.2100\n",
      "Time for epoch 36 is 48.6280 sec\n",
      "Epoch 37, gen_loss: 178.5691, disc_loss: -1.6956\n",
      "Time for epoch 37 is 48.3908 sec\n",
      "Epoch 38, gen_loss: 152.4258, disc_loss: -1.9168\n",
      "Time for epoch 38 is 48.6108 sec\n",
      "Epoch 39, gen_loss: 153.2961, disc_loss: -2.1829\n",
      "Time for epoch 39 is 48.3752 sec\n",
      "Epoch 40, gen_loss: 184.6627, disc_loss: -1.9551\n",
      "Time for epoch 40 is 48.7210 sec\n",
      "Epoch 41, gen_loss: 151.7081, disc_loss: -1.7214\n",
      "Time for epoch 41 is 48.5951 sec\n",
      "Epoch 42, gen_loss: 166.2455, disc_loss: -2.0430\n",
      "Time for epoch 42 is 48.6121 sec\n",
      "Epoch 43, gen_loss: 177.5856, disc_loss: -2.0196\n",
      "Time for epoch 43 is 48.4388 sec\n",
      "Epoch 44, gen_loss: 249.7843, disc_loss: -1.9684\n",
      "Time for epoch 44 is 48.1571 sec\n",
      "Epoch 45, gen_loss: 158.6325, disc_loss: -2.2234\n",
      "Time for epoch 45 is 48.5019 sec\n",
      "Epoch 46, gen_loss: 170.1225, disc_loss: -2.3495\n",
      "Time for epoch 46 is 48.3772 sec\n",
      "Epoch 47, gen_loss: 204.2573, disc_loss: -1.8595\n",
      "Time for epoch 47 is 48.7045 sec\n",
      "Epoch 48, gen_loss: 202.1220, disc_loss: -1.8722\n",
      "Time for epoch 48 is 48.5798 sec\n",
      "Epoch 49, gen_loss: 205.5130, disc_loss: -2.0906\n",
      "Time for epoch 49 is 48.4395 sec\n",
      "Epoch 50, gen_loss: 159.9453, disc_loss: -2.1554\n",
      "Time for epoch 50 is 48.5802 sec\n",
      "Epoch 51, gen_loss: 157.7641, disc_loss: -2.0539\n",
      "Time for epoch 51 is 48.5625 sec\n",
      "Epoch 52, gen_loss: 198.6490, disc_loss: -2.3832\n",
      "Time for epoch 52 is 48.5305 sec\n",
      "Epoch 53, gen_loss: 176.2833, disc_loss: -2.2792\n",
      "Time for epoch 53 is 48.4699 sec\n",
      "Epoch 54, gen_loss: 215.9472, disc_loss: -2.3190\n",
      "Time for epoch 54 is 48.6434 sec\n",
      "Epoch 55, gen_loss: 165.8926, disc_loss: -2.1989\n",
      "Time for epoch 55 is 48.5323 sec\n",
      "Epoch 56, gen_loss: 151.6081, disc_loss: -1.8277\n",
      "Time for epoch 56 is 48.7358 sec\n",
      "Epoch 57, gen_loss: 146.0962, disc_loss: -2.3737\n",
      "Time for epoch 57 is 48.3616 sec\n",
      "Epoch 58, gen_loss: 165.9366, disc_loss: -2.1676\n",
      "Time for epoch 58 is 48.5716 sec\n",
      "Epoch 59, gen_loss: 159.8337, disc_loss: -2.4308\n",
      "Time for epoch 59 is 48.3448 sec\n",
      "Epoch 60, gen_loss: 170.9544, disc_loss: -1.9750\n",
      "Time for epoch 60 is 48.4712 sec\n",
      "Epoch 61, gen_loss: 153.0754, disc_loss: -2.5940\n",
      "Time for epoch 61 is 48.3623 sec\n",
      "Epoch 62, gen_loss: 179.9889, disc_loss: -2.4876\n",
      "Time for epoch 62 is 48.3650 sec\n",
      "Epoch 63, gen_loss: 147.2814, disc_loss: -2.1354\n",
      "Time for epoch 63 is 48.4080 sec\n",
      "Epoch 64, gen_loss: 168.1174, disc_loss: -2.3023\n",
      "Time for epoch 64 is 48.4085 sec\n",
      "Epoch 65, gen_loss: 143.9318, disc_loss: -2.2248\n",
      "Time for epoch 65 is 48.2674 sec\n",
      "Epoch 66, gen_loss: 190.4886, disc_loss: -2.1424\n",
      "Time for epoch 66 is 48.2201 sec\n",
      "Epoch 67, gen_loss: 185.7377, disc_loss: -2.0413\n",
      "Time for epoch 67 is 48.4687 sec\n",
      "Epoch 68, gen_loss: 181.9807, disc_loss: -2.2668\n",
      "Time for epoch 68 is 48.5649 sec\n",
      "Epoch 69, gen_loss: 149.0458, disc_loss: -2.1748\n",
      "Time for epoch 69 is 48.1869 sec\n",
      "Epoch 70, gen_loss: 144.1082, disc_loss: -2.0961\n",
      "Time for epoch 70 is 48.4216 sec\n",
      "Epoch 71, gen_loss: 148.6020, disc_loss: -2.1598\n",
      "Time for epoch 71 is 48.5318 sec\n",
      "Epoch 72, gen_loss: 141.9478, disc_loss: -2.4444\n",
      "Time for epoch 72 is 48.4852 sec\n",
      "Epoch 73, gen_loss: 170.1513, disc_loss: -2.4277\n",
      "Time for epoch 73 is 48.3448 sec\n",
      "Epoch 74, gen_loss: 167.4760, disc_loss: -2.3338\n",
      "Time for epoch 74 is 48.3459 sec\n",
      "Epoch 75, gen_loss: 139.7987, disc_loss: -2.3502\n",
      "Time for epoch 75 is 48.4074 sec\n",
      "Epoch 76, gen_loss: 162.1896, disc_loss: -2.0719\n",
      "Time for epoch 76 is 48.6754 sec\n",
      "Epoch 77, gen_loss: 150.4614, disc_loss: -2.2321\n",
      "Time for epoch 77 is 48.5927 sec\n",
      "Epoch 78, gen_loss: 156.4166, disc_loss: -1.7277\n",
      "Time for epoch 78 is 48.4397 sec\n",
      "Epoch 79, gen_loss: 154.2649, disc_loss: -2.7488\n",
      "Time for epoch 79 is 48.2671 sec\n",
      "Epoch 80, gen_loss: 201.6676, disc_loss: -2.4630\n",
      "Time for epoch 80 is 48.3960 sec\n",
      "Epoch 81, gen_loss: 179.5946, disc_loss: -2.2286\n",
      "Time for epoch 81 is 48.2827 sec\n",
      "Epoch 82, gen_loss: 166.7157, disc_loss: -2.0912\n",
      "Time for epoch 82 is 48.3449 sec\n",
      "Epoch 83, gen_loss: 148.5703, disc_loss: -2.3815\n",
      "Time for epoch 83 is 48.6416 sec\n",
      "Epoch 84, gen_loss: 165.5400, disc_loss: -2.6952\n",
      "Time for epoch 84 is 48.2030 sec\n",
      "Epoch 85, gen_loss: 172.6718, disc_loss: -2.0655\n",
      "Time for epoch 85 is 48.7841 sec\n",
      "Epoch 86, gen_loss: 131.4880, disc_loss: -2.8472\n",
      "Time for epoch 86 is 48.0621 sec\n",
      "Epoch 87, gen_loss: 144.8024, disc_loss: -2.9408\n",
      "Time for epoch 87 is 48.6426 sec\n",
      "Epoch 88, gen_loss: 165.1711, disc_loss: -2.6687\n",
      "Time for epoch 88 is 48.3754 sec\n",
      "Epoch 89, gen_loss: 141.2108, disc_loss: -2.0786\n",
      "Time for epoch 89 is 48.4533 sec\n",
      "Epoch 90, gen_loss: 165.4614, disc_loss: -2.4885\n",
      "Time for epoch 90 is 48.9679 sec\n",
      "Epoch 91, gen_loss: 144.8994, disc_loss: -2.6242\n",
      "Time for epoch 91 is 48.5028 sec\n",
      "Epoch 92, gen_loss: 155.1005, disc_loss: -2.2361\n",
      "Time for epoch 92 is 48.6430 sec\n",
      "Epoch 93, gen_loss: 183.1200, disc_loss: -2.0684\n",
      "Time for epoch 93 is 48.7194 sec\n",
      "Epoch 94, gen_loss: 141.4701, disc_loss: -2.7302\n",
      "Time for epoch 94 is 48.7664 sec\n",
      "Epoch 95, gen_loss: 178.7020, disc_loss: -2.5471\n",
      "Time for epoch 95 is 48.5154 sec\n",
      "Epoch 96, gen_loss: 159.1109, disc_loss: -2.4781\n",
      "Time for epoch 96 is 48.5231 sec\n",
      "Epoch 97, gen_loss: 146.6519, disc_loss: -1.9263\n",
      "Time for epoch 97 is 48.3612 sec\n",
      "Epoch 98, gen_loss: 161.4842, disc_loss: -2.6172\n",
      "Time for epoch 98 is 48.3600 sec\n",
      "Epoch 99, gen_loss: 150.4836, disc_loss: -3.1757\n",
      "Time for epoch 99 is 48.5928 sec\n",
      "Epoch 100, gen_loss: 168.2897, disc_loss: -2.5690\n",
      "Time for epoch 100 is 48.2979 sec\n",
      "Epoch 101, gen_loss: 194.7176, disc_loss: -2.0722\n",
      "Time for epoch 101 is 48.2208 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102, gen_loss: 154.4780, disc_loss: -2.3942\n",
      "Time for epoch 102 is 48.7658 sec\n",
      "Epoch 103, gen_loss: 112.4903, disc_loss: -3.0792\n",
      "Time for epoch 103 is 48.5020 sec\n",
      "Epoch 104, gen_loss: 165.7531, disc_loss: -2.4847\n",
      "Time for epoch 104 is 48.6252 sec\n",
      "Epoch 105, gen_loss: 147.8562, disc_loss: -1.7436\n",
      "Time for epoch 105 is 48.5803 sec\n",
      "Epoch 106, gen_loss: 137.5394, disc_loss: -2.3192\n",
      "Time for epoch 106 is 48.7511 sec\n",
      "Epoch 107, gen_loss: 166.5127, disc_loss: -1.7339\n",
      "Time for epoch 107 is 48.5685 sec\n",
      "Epoch 108, gen_loss: 159.8926, disc_loss: -2.5054\n",
      "Time for epoch 108 is 48.7315 sec\n",
      "Epoch 109, gen_loss: 121.6070, disc_loss: -2.5823\n",
      "Time for epoch 109 is 48.6885 sec\n",
      "Epoch 110, gen_loss: 155.7104, disc_loss: -2.3823\n",
      "Time for epoch 110 is 48.7518 sec\n",
      "Epoch 111, gen_loss: 158.3325, disc_loss: -2.3016\n",
      "Time for epoch 111 is 48.5162 sec\n",
      "Epoch 112, gen_loss: 143.6039, disc_loss: -2.4300\n",
      "Time for epoch 112 is 48.6574 sec\n",
      "Epoch 113, gen_loss: 114.4057, disc_loss: -2.6171\n",
      "Time for epoch 113 is 48.5633 sec\n",
      "Epoch 114, gen_loss: 134.7015, disc_loss: -2.4758\n",
      "Time for epoch 114 is 48.2678 sec\n",
      "Epoch 115, gen_loss: 157.9668, disc_loss: -2.1724\n",
      "Time for epoch 115 is 48.8612 sec\n",
      "Epoch 116, gen_loss: 121.9435, disc_loss: -2.7591\n",
      "Time for epoch 116 is 48.6578 sec\n",
      "Epoch 117, gen_loss: 143.3409, disc_loss: -2.5278\n",
      "Time for epoch 117 is 48.6067 sec\n",
      "Epoch 118, gen_loss: 162.4876, disc_loss: -2.4140\n",
      "Time for epoch 118 is 48.7702 sec\n",
      "Epoch 119, gen_loss: 156.0925, disc_loss: -2.8907\n",
      "Time for epoch 119 is 49.2195 sec\n",
      "Epoch 120, gen_loss: 144.9049, disc_loss: -2.6750\n",
      "Time for epoch 120 is 48.6109 sec\n",
      "Epoch 121, gen_loss: 122.6561, disc_loss: -2.6658\n",
      "Time for epoch 121 is 48.6762 sec\n",
      "Epoch 122, gen_loss: 137.3432, disc_loss: -2.7373\n",
      "Time for epoch 122 is 48.8252 sec\n",
      "Epoch 123, gen_loss: 137.4949, disc_loss: -2.6743\n",
      "Time for epoch 123 is 48.3936 sec\n",
      "Epoch 124, gen_loss: 123.0168, disc_loss: -3.5693\n",
      "Time for epoch 124 is 48.6549 sec\n",
      "Epoch 125, gen_loss: 144.7221, disc_loss: -2.5911\n",
      "Time for epoch 125 is 48.6406 sec\n",
      "Epoch 126, gen_loss: 116.5333, disc_loss: -2.3237\n",
      "Time for epoch 126 is 48.7196 sec\n",
      "Epoch 127, gen_loss: 112.1847, disc_loss: -2.4989\n",
      "Time for epoch 127 is 48.5167 sec\n",
      "Epoch 128, gen_loss: 124.5461, disc_loss: -3.0211\n",
      "Time for epoch 128 is 48.7992 sec\n",
      "Epoch 129, gen_loss: 120.6863, disc_loss: -2.5766\n",
      "Time for epoch 129 is 48.6891 sec\n",
      "Epoch 130, gen_loss: 111.7371, disc_loss: -2.9031\n",
      "Time for epoch 130 is 48.4786 sec\n",
      "Epoch 131, gen_loss: 153.0555, disc_loss: -2.5401\n",
      "Time for epoch 131 is 48.6427 sec\n",
      "Epoch 132, gen_loss: 109.9288, disc_loss: -2.6844\n",
      "Time for epoch 132 is 48.5792 sec\n",
      "Epoch 133, gen_loss: 128.9605, disc_loss: -2.5038\n",
      "Time for epoch 133 is 48.6106 sec\n",
      "Epoch 134, gen_loss: 117.0651, disc_loss: -2.5712\n",
      "Time for epoch 134 is 48.6593 sec\n",
      "Epoch 135, gen_loss: 114.9350, disc_loss: -3.3810\n",
      "Time for epoch 135 is 48.4703 sec\n",
      "Epoch 136, gen_loss: 121.4550, disc_loss: -3.0592\n",
      "Time for epoch 136 is 48.4697 sec\n",
      "Epoch 137, gen_loss: 129.2211, disc_loss: -2.6734\n",
      "Time for epoch 137 is 48.4381 sec\n",
      "Epoch 138, gen_loss: 135.9662, disc_loss: -2.0918\n",
      "Time for epoch 138 is 48.5007 sec\n",
      "Epoch 139, gen_loss: 107.6521, disc_loss: -2.6454\n",
      "Time for epoch 139 is 48.3298 sec\n",
      "Epoch 140, gen_loss: 112.3430, disc_loss: -3.2732\n",
      "Time for epoch 140 is 48.3927 sec\n",
      "Epoch 141, gen_loss: 114.5550, disc_loss: -2.6957\n",
      "Time for epoch 141 is 48.4703 sec\n",
      "Epoch 142, gen_loss: 126.1726, disc_loss: -2.9169\n",
      "Time for epoch 142 is 48.7522 sec\n",
      "Epoch 143, gen_loss: 112.8399, disc_loss: -2.5875\n",
      "Time for epoch 143 is 48.7193 sec\n",
      "Epoch 144, gen_loss: 126.4108, disc_loss: -3.1680\n",
      "Time for epoch 144 is 48.8463 sec\n",
      "Epoch 145, gen_loss: 115.9985, disc_loss: -3.0516\n",
      "Time for epoch 145 is 48.6407 sec\n",
      "Epoch 146, gen_loss: 118.9720, disc_loss: -3.2018\n",
      "Time for epoch 146 is 48.7055 sec\n",
      "Epoch 147, gen_loss: 150.7705, disc_loss: -2.7587\n",
      "Time for epoch 147 is 48.7083 sec\n",
      "Epoch 148, gen_loss: 132.4570, disc_loss: -2.8379\n",
      "Time for epoch 148 is 48.9868 sec\n",
      "Epoch 149, gen_loss: 127.5417, disc_loss: -2.8502\n",
      "Time for epoch 149 is 48.8450 sec\n",
      "Epoch 150, gen_loss: 125.3433, disc_loss: -2.9637\n",
      "Time for epoch 150 is 48.6575 sec\n",
      "Epoch 151, gen_loss: 106.6782, disc_loss: -3.1919\n",
      "Time for epoch 151 is 48.5175 sec\n",
      "Epoch 152, gen_loss: 130.1093, disc_loss: -2.9777\n",
      "Time for epoch 152 is 48.6730 sec\n",
      "Epoch 153, gen_loss: 117.2006, disc_loss: -2.9594\n",
      "Time for epoch 153 is 48.6890 sec\n",
      "Epoch 154, gen_loss: 131.5521, disc_loss: -2.4585\n",
      "Time for epoch 154 is 48.7665 sec\n",
      "Epoch 155, gen_loss: 135.8146, disc_loss: -3.1244\n",
      "Time for epoch 155 is 48.7673 sec\n",
      "Epoch 156, gen_loss: 134.7940, disc_loss: -2.4311\n",
      "Time for epoch 156 is 48.2666 sec\n",
      "Epoch 157, gen_loss: 127.1211, disc_loss: -2.2808\n",
      "Time for epoch 157 is 48.1885 sec\n",
      "Epoch 158, gen_loss: 109.5845, disc_loss: -2.9920\n",
      "Time for epoch 158 is 48.6749 sec\n",
      "Epoch 159, gen_loss: 122.0398, disc_loss: -2.9337\n",
      "Time for epoch 159 is 48.7202 sec\n",
      "Epoch 160, gen_loss: 125.1804, disc_loss: -3.0929\n",
      "Time for epoch 160 is 49.0009 sec\n",
      "Epoch 161, gen_loss: 122.0292, disc_loss: -3.6994\n",
      "Time for epoch 161 is 48.7049 sec\n",
      "Epoch 162, gen_loss: 132.3441, disc_loss: -2.9884\n",
      "Time for epoch 162 is 48.9071 sec\n",
      "Epoch 163, gen_loss: 125.9202, disc_loss: -3.3875\n",
      "Time for epoch 163 is 48.7663 sec\n",
      "Epoch 164, gen_loss: 112.9714, disc_loss: -2.7243\n",
      "Time for epoch 164 is 48.7837 sec\n",
      "Epoch 165, gen_loss: 114.0204, disc_loss: -3.1173\n",
      "Time for epoch 165 is 49.1087 sec\n",
      "Epoch 166, gen_loss: 120.0929, disc_loss: -2.2125\n",
      "Time for epoch 166 is 48.7674 sec\n",
      "Epoch 167, gen_loss: 115.0884, disc_loss: -2.4935\n",
      "Time for epoch 167 is 48.9072 sec\n",
      "Epoch 168, gen_loss: 129.9374, disc_loss: -2.5630\n",
      "Time for epoch 168 is 48.9240 sec\n",
      "Epoch 169, gen_loss: 119.6830, disc_loss: -3.1504\n",
      "Time for epoch 169 is 48.7733 sec\n",
      "Epoch 170, gen_loss: 116.6489, disc_loss: -2.7926\n",
      "Time for epoch 170 is 48.6873 sec\n",
      "Epoch 171, gen_loss: 103.3425, disc_loss: -3.5228\n",
      "Time for epoch 171 is 48.5010 sec\n",
      "Epoch 172, gen_loss: 137.9175, disc_loss: -3.0670\n",
      "Time for epoch 172 is 48.7352 sec\n",
      "Epoch 173, gen_loss: 111.2058, disc_loss: -2.8929\n",
      "Time for epoch 173 is 48.9239 sec\n",
      "Epoch 174, gen_loss: 105.8084, disc_loss: -2.8388\n",
      "Time for epoch 174 is 48.8910 sec\n",
      "Epoch 175, gen_loss: 116.8007, disc_loss: -3.1006\n",
      "Time for epoch 175 is 48.7032 sec\n",
      "Epoch 176, gen_loss: 116.0630, disc_loss: -3.3340\n",
      "Time for epoch 176 is 48.8444 sec\n",
      "Epoch 177, gen_loss: 118.2691, disc_loss: -2.7183\n",
      "Time for epoch 177 is 48.7977 sec\n",
      "Epoch 178, gen_loss: 103.9863, disc_loss: -2.6644\n",
      "Time for epoch 178 is 48.9389 sec\n",
      "Epoch 179, gen_loss: 113.7199, disc_loss: -2.8867\n",
      "Time for epoch 179 is 48.5623 sec\n",
      "Epoch 180, gen_loss: 126.0420, disc_loss: -2.3318\n",
      "Time for epoch 180 is 48.5946 sec\n",
      "Epoch 181, gen_loss: 99.2613, disc_loss: -2.8339\n",
      "Time for epoch 181 is 48.4870 sec\n",
      "Epoch 182, gen_loss: 112.2590, disc_loss: -3.3907\n",
      "Time for epoch 182 is 48.5448 sec\n",
      "Epoch 183, gen_loss: 107.6851, disc_loss: -2.7667\n",
      "Time for epoch 183 is 48.6122 sec\n",
      "Epoch 184, gen_loss: 112.8453, disc_loss: -2.8652\n",
      "Time for epoch 184 is 49.0334 sec\n",
      "Epoch 185, gen_loss: 112.2516, disc_loss: -3.5252\n",
      "Time for epoch 185 is 48.5311 sec\n",
      "Epoch 186, gen_loss: 174.0419, disc_loss: -2.2361\n",
      "Time for epoch 186 is 49.0630 sec\n",
      "Epoch 187, gen_loss: 103.5532, disc_loss: -3.1237\n",
      "Time for epoch 187 is 48.7203 sec\n",
      "Epoch 188, gen_loss: 125.5108, disc_loss: -2.1302\n",
      "Time for epoch 188 is 48.8918 sec\n",
      "Epoch 189, gen_loss: 148.0012, disc_loss: -2.6794\n",
      "Time for epoch 189 is 48.6584 sec\n",
      "Epoch 190, gen_loss: 109.3107, disc_loss: -3.5716\n",
      "Time for epoch 190 is 48.5320 sec\n",
      "Epoch 191, gen_loss: 111.9192, disc_loss: -2.6393\n",
      "Time for epoch 191 is 48.5190 sec\n",
      "Epoch 192, gen_loss: 110.3603, disc_loss: -3.8935\n",
      "Time for epoch 192 is 48.5639 sec\n",
      "Epoch 193, gen_loss: 107.1568, disc_loss: -3.4659\n",
      "Time for epoch 193 is 48.2507 sec\n",
      "Epoch 194, gen_loss: 112.7798, disc_loss: -2.9912\n",
      "Time for epoch 194 is 48.5491 sec\n",
      "Epoch 195, gen_loss: 112.5748, disc_loss: -3.7996\n",
      "Time for epoch 195 is 48.5184 sec\n",
      "Epoch 196, gen_loss: 110.9237, disc_loss: -3.5059\n",
      "Time for epoch 196 is 48.8131 sec\n",
      "Epoch 197, gen_loss: 114.5204, disc_loss: -3.0116\n",
      "Time for epoch 197 is 48.4403 sec\n",
      "Epoch 198, gen_loss: 111.0051, disc_loss: -3.3555\n",
      "Time for epoch 198 is 48.3444 sec\n",
      "Epoch 199, gen_loss: 121.2933, disc_loss: -2.7054\n",
      "Time for epoch 199 is 46.4860 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, gen_loss: 100.6584, disc_loss: -3.5332\n",
      "Time for epoch 200 is 48.9852 sec\n",
      "Epoch 201, gen_loss: 101.0673, disc_loss: -3.8646\n",
      "Time for epoch 201 is 48.8467 sec\n",
      "Epoch 202, gen_loss: 112.4770, disc_loss: -3.4472\n",
      "Time for epoch 202 is 48.6635 sec\n",
      "Epoch 203, gen_loss: 108.5373, disc_loss: -3.3956\n",
      "Time for epoch 203 is 48.9497 sec\n",
      "Epoch 204, gen_loss: 101.2233, disc_loss: -3.7532\n",
      "Time for epoch 204 is 48.3617 sec\n",
      "Epoch 205, gen_loss: 112.4683, disc_loss: -3.1793\n",
      "Time for epoch 205 is 48.8854 sec\n",
      "Epoch 206, gen_loss: 107.4842, disc_loss: -3.7977\n",
      "Time for epoch 206 is 48.8442 sec\n",
      "Epoch 207, gen_loss: 102.0517, disc_loss: -3.6464\n",
      "Time for epoch 207 is 48.7054 sec\n",
      "Epoch 208, gen_loss: 105.1914, disc_loss: -3.0490\n",
      "Time for epoch 208 is 48.5482 sec\n",
      "Epoch 209, gen_loss: 96.0962, disc_loss: -2.9467\n",
      "Time for epoch 209 is 48.4736 sec\n",
      "Epoch 210, gen_loss: 101.1015, disc_loss: -3.8167\n",
      "Time for epoch 210 is 48.7319 sec\n",
      "Epoch 211, gen_loss: 104.5750, disc_loss: -3.1205\n",
      "Time for epoch 211 is 48.5164 sec\n",
      "Epoch 212, gen_loss: 116.0723, disc_loss: -3.7120\n",
      "Time for epoch 212 is 48.5327 sec\n",
      "Epoch 213, gen_loss: 112.6256, disc_loss: -3.3119\n",
      "Time for epoch 213 is 48.2827 sec\n",
      "Epoch 214, gen_loss: 110.9592, disc_loss: -3.7095\n",
      "Time for epoch 214 is 49.4379 sec\n",
      "Epoch 215, gen_loss: 110.6230, disc_loss: -3.6155\n",
      "Time for epoch 215 is 48.5789 sec\n",
      "Epoch 216, gen_loss: 114.9828, disc_loss: -3.3366\n",
      "Time for epoch 216 is 48.6109 sec\n",
      "Epoch 217, gen_loss: 136.2707, disc_loss: -3.5672\n",
      "Time for epoch 217 is 48.3603 sec\n",
      "Epoch 218, gen_loss: 94.2194, disc_loss: -3.5152\n",
      "Time for epoch 218 is 48.2672 sec\n",
      "Epoch 219, gen_loss: 101.6134, disc_loss: -3.9525\n",
      "Time for epoch 219 is 48.5340 sec\n",
      "Epoch 220, gen_loss: 124.9755, disc_loss: -2.9868\n",
      "Time for epoch 220 is 48.7309 sec\n",
      "Epoch 221, gen_loss: 96.7528, disc_loss: -3.8560\n",
      "Time for epoch 221 is 48.1416 sec\n",
      "Epoch 222, gen_loss: 97.0989, disc_loss: -4.0656\n",
      "Time for epoch 222 is 48.6112 sec\n",
      "Epoch 223, gen_loss: 105.8515, disc_loss: -2.1790\n",
      "Time for epoch 223 is 48.5318 sec\n",
      "Epoch 224, gen_loss: 107.7014, disc_loss: -3.1158\n",
      "Time for epoch 224 is 48.1280 sec\n",
      "Epoch 225, gen_loss: 106.9003, disc_loss: -2.9340\n",
      "Time for epoch 225 is 48.0498 sec\n",
      "Epoch 226, gen_loss: 90.8169, disc_loss: -3.7385\n",
      "Time for epoch 226 is 48.1174 sec\n",
      "Epoch 227, gen_loss: 102.8253, disc_loss: -3.6544\n",
      "Time for epoch 227 is 47.7980 sec\n",
      "Epoch 228, gen_loss: 105.1132, disc_loss: -3.3138\n",
      "Time for epoch 228 is 48.3931 sec\n",
      "Epoch 229, gen_loss: 98.7527, disc_loss: -3.3419\n",
      "Time for epoch 229 is 47.8588 sec\n",
      "Epoch 230, gen_loss: 95.3217, disc_loss: -3.4314\n",
      "Time for epoch 230 is 48.0640 sec\n",
      "Epoch 231, gen_loss: 100.1634, disc_loss: -3.0176\n",
      "Time for epoch 231 is 48.5629 sec\n",
      "Epoch 232, gen_loss: 102.7100, disc_loss: -3.5717\n",
      "Time for epoch 232 is 48.6277 sec\n",
      "Epoch 233, gen_loss: 93.2834, disc_loss: -3.6503\n",
      "Time for epoch 233 is 48.7181 sec\n",
      "Epoch 234, gen_loss: 104.7646, disc_loss: -3.3541\n",
      "Time for epoch 234 is 48.5330 sec\n",
      "Epoch 235, gen_loss: 95.3535, disc_loss: -3.9129\n",
      "Time for epoch 235 is 48.6736 sec\n",
      "Epoch 236, gen_loss: 101.3074, disc_loss: -3.6794\n",
      "Time for epoch 236 is 48.8451 sec\n",
      "Epoch 237, gen_loss: 131.7191, disc_loss: -3.1724\n",
      "Time for epoch 237 is 48.3932 sec\n",
      "Epoch 238, gen_loss: 91.3874, disc_loss: -3.6026\n",
      "Time for epoch 238 is 48.4714 sec\n",
      "Epoch 239, gen_loss: 97.8534, disc_loss: -3.8965\n",
      "Time for epoch 239 is 48.4391 sec\n",
      "Epoch 240, gen_loss: 95.7733, disc_loss: -3.9403\n",
      "Time for epoch 240 is 48.3760 sec\n",
      "Epoch 241, gen_loss: 93.0207, disc_loss: -3.3257\n",
      "Time for epoch 241 is 48.1261 sec\n",
      "Epoch 242, gen_loss: 98.3184, disc_loss: -3.9533\n",
      "Time for epoch 242 is 48.3462 sec\n",
      "Epoch 243, gen_loss: 85.1087, disc_loss: -3.6821\n",
      "Time for epoch 243 is 48.3769 sec\n",
      "Epoch 244, gen_loss: 100.7152, disc_loss: -3.4448\n",
      "Time for epoch 244 is 48.9067 sec\n",
      "Epoch 245, gen_loss: 95.1767, disc_loss: -3.6847\n",
      "Time for epoch 245 is 48.4997 sec\n",
      "Epoch 246, gen_loss: 92.9830, disc_loss: -4.1010\n",
      "Time for epoch 246 is 48.0159 sec\n",
      "Epoch 247, gen_loss: 92.4364, disc_loss: -3.9328\n",
      "Time for epoch 247 is 48.2050 sec\n",
      "Epoch 248, gen_loss: 103.0557, disc_loss: -3.8044\n",
      "Time for epoch 248 is 48.5649 sec\n",
      "Epoch 249, gen_loss: 97.0623, disc_loss: -3.0196\n",
      "Time for epoch 249 is 48.3139 sec\n",
      "Epoch 250, gen_loss: 88.1388, disc_loss: -4.2885\n",
      "Time for epoch 250 is 48.5482 sec\n",
      "Epoch 251, gen_loss: 87.8618, disc_loss: -4.0883\n",
      "Time for epoch 251 is 48.1870 sec\n",
      "Epoch 252, gen_loss: 106.1075, disc_loss: -3.4477\n",
      "Time for epoch 252 is 48.3150 sec\n",
      "Epoch 253, gen_loss: 102.0389, disc_loss: -4.1545\n",
      "Time for epoch 253 is 48.4670 sec\n",
      "Epoch 254, gen_loss: 84.6025, disc_loss: -4.1767\n",
      "Time for epoch 254 is 48.3422 sec\n",
      "Epoch 255, gen_loss: 104.3312, disc_loss: -3.4261\n",
      "Time for epoch 255 is 48.2678 sec\n",
      "Epoch 256, gen_loss: 89.8755, disc_loss: -3.9869\n",
      "Time for epoch 256 is 48.5321 sec\n",
      "Epoch 257, gen_loss: 95.7770, disc_loss: -3.5142\n",
      "Time for epoch 257 is 48.4392 sec\n",
      "Epoch 258, gen_loss: 87.4884, disc_loss: -3.5216\n",
      "Time for epoch 258 is 48.7355 sec\n",
      "Epoch 259, gen_loss: 100.2818, disc_loss: -2.6984\n",
      "Time for epoch 259 is 48.5167 sec\n",
      "Epoch 260, gen_loss: 88.4511, disc_loss: -4.1599\n",
      "Time for epoch 260 is 48.5641 sec\n",
      "Epoch 261, gen_loss: 89.4780, disc_loss: -3.7443\n",
      "Time for epoch 261 is 48.3776 sec\n",
      "Epoch 262, gen_loss: 92.5526, disc_loss: -4.1671\n",
      "Time for epoch 262 is 48.5158 sec\n",
      "Epoch 263, gen_loss: 83.8793, disc_loss: -4.1212\n",
      "Time for epoch 263 is 48.7366 sec\n",
      "Epoch 264, gen_loss: 102.2324, disc_loss: -4.3908\n",
      "Time for epoch 264 is 48.3749 sec\n",
      "Epoch 265, gen_loss: 98.3799, disc_loss: -4.0939\n",
      "Time for epoch 265 is 48.3277 sec\n",
      "Epoch 266, gen_loss: 117.7270, disc_loss: -3.2594\n",
      "Time for epoch 266 is 48.2219 sec\n",
      "Epoch 267, gen_loss: 112.3070, disc_loss: -3.4468\n",
      "Time for epoch 267 is 48.3458 sec\n",
      "Epoch 268, gen_loss: 89.5117, disc_loss: -3.8742\n",
      "Time for epoch 268 is 48.6890 sec\n",
      "Epoch 269, gen_loss: 101.4916, disc_loss: -3.9997\n",
      "Time for epoch 269 is 48.4244 sec\n",
      "Epoch 270, gen_loss: 96.1790, disc_loss: -3.5200\n",
      "Time for epoch 270 is 48.3592 sec\n",
      "Epoch 271, gen_loss: 92.7087, disc_loss: -3.7788\n",
      "Time for epoch 271 is 48.1411 sec\n",
      "Epoch 272, gen_loss: 87.4334, disc_loss: -4.3055\n",
      "Time for epoch 272 is 48.2209 sec\n",
      "Epoch 273, gen_loss: 94.0897, disc_loss: -4.0527\n",
      "Time for epoch 273 is 48.5273 sec\n",
      "Epoch 274, gen_loss: 89.8167, disc_loss: -4.2314\n",
      "Time for epoch 274 is 48.1262 sec\n",
      "Epoch 275, gen_loss: 106.3295, disc_loss: -3.5873\n",
      "Time for epoch 275 is 48.2971 sec\n",
      "Epoch 276, gen_loss: 93.3241, disc_loss: -4.5653\n",
      "Time for epoch 276 is 47.2506 sec\n",
      "Epoch 277, gen_loss: 105.9109, disc_loss: -3.4100\n",
      "Time for epoch 277 is 48.3925 sec\n",
      "Epoch 278, gen_loss: 89.4790, disc_loss: -4.5446\n",
      "Time for epoch 278 is 48.4483 sec\n",
      "Epoch 279, gen_loss: 92.2937, disc_loss: -4.1299\n",
      "Time for epoch 279 is 48.4925 sec\n",
      "Epoch 280, gen_loss: 83.0184, disc_loss: -4.4865\n",
      "Time for epoch 280 is 48.1881 sec\n",
      "Epoch 281, gen_loss: 98.4270, disc_loss: -3.8182\n",
      "Time for epoch 281 is 48.4853 sec\n",
      "Epoch 282, gen_loss: 90.1370, disc_loss: -4.2771\n",
      "Time for epoch 282 is 48.5629 sec\n",
      "Epoch 283, gen_loss: 88.8405, disc_loss: -4.4057\n",
      "Time for epoch 283 is 48.3465 sec\n",
      "Epoch 284, gen_loss: 93.7236, disc_loss: -4.4143\n",
      "Time for epoch 284 is 48.0166 sec\n",
      "Epoch 285, gen_loss: 98.5589, disc_loss: -4.1490\n",
      "Time for epoch 285 is 48.8439 sec\n",
      "Epoch 286, gen_loss: 94.2531, disc_loss: -4.2359\n",
      "Time for epoch 286 is 48.2986 sec\n",
      "Epoch 287, gen_loss: 111.9981, disc_loss: -3.6378\n",
      "Time for epoch 287 is 48.7355 sec\n",
      "Epoch 288, gen_loss: 101.8395, disc_loss: -4.1259\n",
      "Time for epoch 288 is 48.2516 sec\n",
      "Epoch 289, gen_loss: 86.8415, disc_loss: -4.1598\n",
      "Time for epoch 289 is 48.3291 sec\n",
      "Epoch 290, gen_loss: 79.5905, disc_loss: -4.2219\n",
      "Time for epoch 290 is 48.7173 sec\n",
      "Epoch 291, gen_loss: 98.2442, disc_loss: -4.1920\n",
      "Time for epoch 291 is 48.0038 sec\n",
      "Epoch 292, gen_loss: 80.2543, disc_loss: -4.3018\n",
      "Time for epoch 292 is 48.3798 sec\n",
      "Epoch 293, gen_loss: 93.4946, disc_loss: -4.0773\n",
      "Time for epoch 293 is 48.0081 sec\n",
      "Epoch 294, gen_loss: 95.1243, disc_loss: -3.9177\n",
      "Time for epoch 294 is 48.2653 sec\n",
      "Epoch 295, gen_loss: 92.5055, disc_loss: -3.8107\n",
      "Time for epoch 295 is 48.2980 sec\n",
      "Epoch 296, gen_loss: 90.4897, disc_loss: -4.4776\n",
      "Time for epoch 296 is 47.8491 sec\n",
      "Epoch 297, gen_loss: 81.7004, disc_loss: -4.6890\n",
      "Time for epoch 297 is 48.1246 sec\n",
      "Epoch 298, gen_loss: 90.6038, disc_loss: -4.6924\n",
      "Time for epoch 298 is 48.2963 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299, gen_loss: 87.5514, disc_loss: -4.4146\n",
      "Time for epoch 299 is 48.3628 sec\n",
      "Epoch 300, gen_loss: 86.1360, disc_loss: -4.5773\n",
      "Time for epoch 300 is 48.1120 sec\n",
      "Epoch 301, gen_loss: 87.7002, disc_loss: -3.9455\n",
      "Time for epoch 301 is 48.2321 sec\n",
      "Epoch 302, gen_loss: 81.5665, disc_loss: -4.5768\n",
      "Time for epoch 302 is 48.3752 sec\n",
      "Epoch 303, gen_loss: 96.2230, disc_loss: -4.1643\n",
      "Time for epoch 303 is 47.8107 sec\n",
      "Epoch 304, gen_loss: 76.3990, disc_loss: -4.3284\n",
      "Time for epoch 304 is 48.4546 sec\n",
      "Epoch 305, gen_loss: 93.5598, disc_loss: -4.5781\n",
      "Time for epoch 305 is 48.0325 sec\n",
      "Epoch 306, gen_loss: 89.4360, disc_loss: -4.7746\n",
      "Time for epoch 306 is 48.2981 sec\n",
      "Epoch 307, gen_loss: 89.5362, disc_loss: -3.8404\n",
      "Time for epoch 307 is 48.0472 sec\n",
      "Epoch 308, gen_loss: 100.5983, disc_loss: -3.6760\n",
      "Time for epoch 308 is 48.1740 sec\n",
      "Epoch 309, gen_loss: 85.0481, disc_loss: -4.4533\n",
      "Time for epoch 309 is 48.5942 sec\n",
      "Epoch 310, gen_loss: 85.6473, disc_loss: -4.5528\n",
      "Time for epoch 310 is 47.9396 sec\n",
      "Epoch 311, gen_loss: 87.6065, disc_loss: -4.2160\n",
      "Time for epoch 311 is 48.2665 sec\n",
      "Epoch 312, gen_loss: 88.7310, disc_loss: -4.4969\n",
      "Time for epoch 312 is 47.9379 sec\n",
      "Epoch 313, gen_loss: 79.7894, disc_loss: -4.4512\n",
      "Time for epoch 313 is 47.8102 sec\n",
      "Epoch 314, gen_loss: 85.1646, disc_loss: -4.6379\n",
      "Time for epoch 314 is 48.0804 sec\n",
      "Epoch 315, gen_loss: 98.2578, disc_loss: -4.4175\n",
      "Time for epoch 315 is 48.0161 sec\n",
      "Epoch 316, gen_loss: 93.2006, disc_loss: -4.3870\n",
      "Time for epoch 316 is 47.9698 sec\n",
      "Epoch 317, gen_loss: 88.9512, disc_loss: -4.7213\n",
      "Time for epoch 317 is 48.0645 sec\n",
      "Epoch 318, gen_loss: 86.5680, disc_loss: -4.9407\n",
      "Time for epoch 318 is 48.3837 sec\n",
      "Epoch 319, gen_loss: 92.0425, disc_loss: -4.8158\n",
      "Time for epoch 319 is 48.2456 sec\n",
      "Epoch 320, gen_loss: 89.1196, disc_loss: -4.5772\n",
      "Time for epoch 320 is 48.1719 sec\n",
      "Epoch 321, gen_loss: 83.0378, disc_loss: -4.5987\n",
      "Time for epoch 321 is 48.1916 sec\n",
      "Epoch 322, gen_loss: 86.8815, disc_loss: -4.8303\n",
      "Time for epoch 322 is 47.9081 sec\n",
      "Epoch 323, gen_loss: 85.8931, disc_loss: -4.7916\n",
      "Time for epoch 323 is 48.3455 sec\n",
      "Epoch 324, gen_loss: 76.3103, disc_loss: -4.6947\n",
      "Time for epoch 324 is 48.0955 sec\n",
      "Epoch 325, gen_loss: 118.6453, disc_loss: -4.1449\n",
      "Time for epoch 325 is 48.3605 sec\n",
      "Epoch 326, gen_loss: 86.8288, disc_loss: -4.0433\n",
      "Time for epoch 326 is 48.0175 sec\n",
      "Epoch 327, gen_loss: 81.6456, disc_loss: -4.3631\n",
      "Time for epoch 327 is 48.0163 sec\n",
      "Epoch 328, gen_loss: 88.8866, disc_loss: -3.9912\n",
      "Time for epoch 328 is 48.2664 sec\n",
      "Epoch 329, gen_loss: 100.4651, disc_loss: -4.2536\n",
      "Time for epoch 329 is 47.9239 sec\n",
      "Epoch 330, gen_loss: 94.4114, disc_loss: -4.5343\n",
      "Time for epoch 330 is 47.9080 sec\n",
      "Epoch 331, gen_loss: 88.5336, disc_loss: -4.6183\n",
      "Time for epoch 331 is 48.1737 sec\n",
      "Epoch 332, gen_loss: 90.8818, disc_loss: -4.6582\n",
      "Time for epoch 332 is 48.1138 sec\n",
      "Epoch 333, gen_loss: 97.8708, disc_loss: -4.1287\n",
      "Time for epoch 333 is 47.9853 sec\n",
      "Epoch 334, gen_loss: 82.1805, disc_loss: -4.2089\n",
      "Time for epoch 334 is 48.2349 sec\n",
      "Epoch 335, gen_loss: 82.7413, disc_loss: -4.8641\n",
      "Time for epoch 335 is 48.0778 sec\n",
      "Epoch 336, gen_loss: 85.7250, disc_loss: -4.0219\n",
      "Time for epoch 336 is 47.9852 sec\n",
      "Epoch 337, gen_loss: 87.6959, disc_loss: -4.1502\n",
      "Time for epoch 337 is 48.1414 sec\n",
      "Epoch 338, gen_loss: 83.3354, disc_loss: -4.6596\n",
      "Time for epoch 338 is 48.2824 sec\n",
      "Epoch 339, gen_loss: 78.2278, disc_loss: -4.7805\n",
      "Time for epoch 339 is 48.2356 sec\n",
      "Epoch 340, gen_loss: 86.2974, disc_loss: -4.3518\n",
      "Time for epoch 340 is 48.2815 sec\n",
      "Epoch 341, gen_loss: 91.2357, disc_loss: -4.0845\n",
      "Time for epoch 341 is 48.2997 sec\n",
      "Epoch 342, gen_loss: 88.2736, disc_loss: -4.5596\n",
      "Time for epoch 342 is 48.1004 sec\n",
      "Epoch 343, gen_loss: 86.0980, disc_loss: -4.3083\n",
      "Time for epoch 343 is 48.0003 sec\n",
      "Epoch 344, gen_loss: 87.2586, disc_loss: -4.3852\n",
      "Time for epoch 344 is 48.1249 sec\n",
      "Epoch 345, gen_loss: 77.9990, disc_loss: -4.8007\n",
      "Time for epoch 345 is 47.9694 sec\n",
      "Epoch 346, gen_loss: 76.0755, disc_loss: -4.7902\n",
      "Time for epoch 346 is 47.9390 sec\n",
      "Epoch 347, gen_loss: 81.5077, disc_loss: -4.8125\n",
      "Time for epoch 347 is 48.3757 sec\n",
      "Epoch 348, gen_loss: 85.9011, disc_loss: -4.5581\n",
      "Time for epoch 348 is 48.0637 sec\n",
      "Epoch 349, gen_loss: 86.3091, disc_loss: -4.6542\n",
      "Time for epoch 349 is 48.0332 sec\n",
      "Epoch 350, gen_loss: 92.4580, disc_loss: -4.5446\n",
      "Time for epoch 350 is 48.2663 sec\n",
      "Epoch 351, gen_loss: 89.3271, disc_loss: -4.4248\n",
      "Time for epoch 351 is 48.0158 sec\n",
      "Epoch 352, gen_loss: 89.6148, disc_loss: -4.6502\n",
      "Time for epoch 352 is 47.8281 sec\n",
      "Epoch 353, gen_loss: 85.0209, disc_loss: -4.3425\n",
      "Time for epoch 353 is 48.2815 sec\n",
      "Epoch 354, gen_loss: 73.9436, disc_loss: -4.2385\n",
      "Time for epoch 354 is 48.5008 sec\n",
      "Epoch 355, gen_loss: 88.6268, disc_loss: -4.3681\n",
      "Time for epoch 355 is 48.1416 sec\n",
      "Epoch 356, gen_loss: 94.4897, disc_loss: -4.7252\n",
      "Time for epoch 356 is 48.2206 sec\n",
      "Epoch 357, gen_loss: 84.7082, disc_loss: -4.6937\n",
      "Time for epoch 357 is 47.9218 sec\n",
      "Epoch 358, gen_loss: 86.2156, disc_loss: -4.7482\n",
      "Time for epoch 358 is 47.7983 sec\n",
      "Epoch 359, gen_loss: 83.9611, disc_loss: -4.8112\n",
      "Time for epoch 359 is 48.0769 sec\n",
      "Epoch 360, gen_loss: 93.1815, disc_loss: -4.7712\n",
      "Time for epoch 360 is 48.1430 sec\n",
      "Epoch 361, gen_loss: 78.4509, disc_loss: -4.4787\n",
      "Time for epoch 361 is 48.1715 sec\n",
      "Epoch 362, gen_loss: 88.2049, disc_loss: -4.3347\n",
      "Time for epoch 362 is 47.9804 sec\n",
      "Epoch 363, gen_loss: 89.0755, disc_loss: -4.8620\n",
      "Time for epoch 363 is 48.5788 sec\n",
      "Epoch 364, gen_loss: 88.6686, disc_loss: -4.6408\n",
      "Time for epoch 364 is 48.1112 sec\n",
      "Epoch 365, gen_loss: 91.9808, disc_loss: -4.7859\n",
      "Time for epoch 365 is 48.0325 sec\n",
      "Epoch 366, gen_loss: 76.8139, disc_loss: -4.8613\n",
      "Time for epoch 366 is 48.0951 sec\n",
      "Epoch 367, gen_loss: 84.0794, disc_loss: -4.9859\n",
      "Time for epoch 367 is 47.8436 sec\n",
      "Epoch 368, gen_loss: 82.3338, disc_loss: -4.9759\n",
      "Time for epoch 368 is 47.8438 sec\n",
      "Epoch 369, gen_loss: 80.1044, disc_loss: -4.8169\n",
      "Time for epoch 369 is 47.9696 sec\n",
      "Epoch 370, gen_loss: 95.9222, disc_loss: -4.4123\n",
      "Time for epoch 370 is 47.7373 sec\n",
      "Epoch 371, gen_loss: 94.9525, disc_loss: -4.2979\n",
      "Time for epoch 371 is 47.8751 sec\n",
      "Epoch 372, gen_loss: 93.7674, disc_loss: -4.3523\n",
      "Time for epoch 372 is 47.7828 sec\n",
      "Epoch 373, gen_loss: 92.3107, disc_loss: -4.5360\n",
      "Time for epoch 373 is 47.7521 sec\n",
      "Epoch 374, gen_loss: 86.7350, disc_loss: -4.8876\n",
      "Time for epoch 374 is 47.7968 sec\n",
      "Epoch 375, gen_loss: 88.9832, disc_loss: -4.6512\n",
      "Time for epoch 375 is 47.8921 sec\n",
      "Epoch 376, gen_loss: 84.7041, disc_loss: -4.8192\n",
      "Time for epoch 376 is 48.2204 sec\n",
      "Epoch 377, gen_loss: 80.1573, disc_loss: -5.0767\n",
      "Time for epoch 377 is 47.8482 sec\n",
      "Epoch 378, gen_loss: 81.4169, disc_loss: -4.9404\n",
      "Time for epoch 378 is 48.2346 sec\n",
      "Epoch 379, gen_loss: 81.9854, disc_loss: -4.9718\n",
      "Time for epoch 379 is 48.2653 sec\n",
      "Epoch 380, gen_loss: 80.3858, disc_loss: -5.0148\n",
      "Time for epoch 380 is 48.1442 sec\n",
      "Epoch 381, gen_loss: 75.3926, disc_loss: -4.9832\n",
      "Time for epoch 381 is 48.3297 sec\n",
      "Epoch 382, gen_loss: 91.9658, disc_loss: -4.6737\n",
      "Time for epoch 382 is 48.1004 sec\n",
      "Epoch 383, gen_loss: 91.7811, disc_loss: -3.7557\n",
      "Time for epoch 383 is 47.9795 sec\n",
      "Epoch 384, gen_loss: 81.6390, disc_loss: -4.1082\n",
      "Time for epoch 384 is 47.9681 sec\n",
      "Epoch 385, gen_loss: 90.3005, disc_loss: -4.9287\n",
      "Time for epoch 385 is 47.9988 sec\n",
      "Epoch 386, gen_loss: 80.7316, disc_loss: -5.0122\n",
      "Time for epoch 386 is 47.9064 sec\n",
      "Epoch 387, gen_loss: 78.9210, disc_loss: -4.4544\n",
      "Time for epoch 387 is 48.0778 sec\n",
      "Epoch 388, gen_loss: 74.2888, disc_loss: -5.1326\n",
      "Time for epoch 388 is 48.2091 sec\n",
      "Epoch 389, gen_loss: 84.7357, disc_loss: -4.6989\n",
      "Time for epoch 389 is 47.9977 sec\n",
      "Epoch 390, gen_loss: 81.5788, disc_loss: -5.3646\n",
      "Time for epoch 390 is 48.0105 sec\n",
      "Epoch 391, gen_loss: 83.9519, disc_loss: -5.2846\n",
      "Time for epoch 391 is 48.0782 sec\n",
      "Epoch 392, gen_loss: 79.9995, disc_loss: -4.8402\n",
      "Time for epoch 392 is 48.0800 sec\n",
      "Epoch 393, gen_loss: 92.7602, disc_loss: -4.3347\n",
      "Time for epoch 393 is 48.6416 sec\n",
      "Epoch 394, gen_loss: 86.0621, disc_loss: -4.8751\n",
      "Time for epoch 394 is 48.2204 sec\n",
      "Epoch 395, gen_loss: 84.7838, disc_loss: -4.9151\n",
      "Time for epoch 395 is 48.1883 sec\n",
      "Epoch 396, gen_loss: 84.4480, disc_loss: -4.7015\n",
      "Time for epoch 396 is 48.2048 sec\n",
      "Epoch 397, gen_loss: 80.3974, disc_loss: -4.8856\n",
      "Time for epoch 397 is 48.2029 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398, gen_loss: 82.8762, disc_loss: -5.2660\n",
      "Time for epoch 398 is 47.6570 sec\n",
      "Epoch 399, gen_loss: 86.6311, disc_loss: -5.0175\n",
      "Time for epoch 399 is 47.7504 sec\n",
      "Epoch 400, gen_loss: 88.9393, disc_loss: -4.7034\n",
      "Time for epoch 400 is 48.1106 sec\n",
      "Epoch 401, gen_loss: 86.5778, disc_loss: -4.7930\n",
      "Time for epoch 401 is 48.0325 sec\n",
      "Epoch 402, gen_loss: 82.3480, disc_loss: -4.9030\n",
      "Time for epoch 402 is 48.2814 sec\n",
      "Epoch 403, gen_loss: 79.5173, disc_loss: -5.2378\n",
      "Time for epoch 403 is 47.9867 sec\n",
      "Epoch 404, gen_loss: 95.1003, disc_loss: -4.3697\n",
      "Time for epoch 404 is 48.0782 sec\n",
      "Epoch 405, gen_loss: 102.9596, disc_loss: -4.1205\n",
      "Time for epoch 405 is 48.1427 sec\n",
      "Epoch 406, gen_loss: 78.0472, disc_loss: -4.7482\n",
      "Time for epoch 406 is 48.1100 sec\n",
      "Epoch 407, gen_loss: 77.2475, disc_loss: -4.8646\n",
      "Time for epoch 407 is 48.2039 sec\n",
      "Epoch 408, gen_loss: 75.8774, disc_loss: -5.3854\n",
      "Time for epoch 408 is 47.7031 sec\n",
      "Epoch 409, gen_loss: 77.5099, disc_loss: -4.8929\n",
      "Time for epoch 409 is 47.5942 sec\n",
      "Epoch 410, gen_loss: 80.5424, disc_loss: -4.4849\n",
      "Time for epoch 410 is 47.9850 sec\n",
      "Epoch 411, gen_loss: 87.2937, disc_loss: -4.5331\n",
      "Time for epoch 411 is 47.9858 sec\n",
      "Epoch 412, gen_loss: 80.6521, disc_loss: -4.9088\n",
      "Time for epoch 412 is 48.1265 sec\n",
      "Epoch 413, gen_loss: 80.4151, disc_loss: -5.0177\n",
      "Time for epoch 413 is 48.0491 sec\n",
      "Epoch 414, gen_loss: 77.2737, disc_loss: -5.3624\n",
      "Time for epoch 414 is 47.6542 sec\n",
      "Epoch 415, gen_loss: 88.9233, disc_loss: -4.6814\n",
      "Time for epoch 415 is 47.9235 sec\n",
      "Epoch 416, gen_loss: 73.7260, disc_loss: -5.1790\n",
      "Time for epoch 416 is 47.9081 sec\n",
      "Epoch 417, gen_loss: 81.2256, disc_loss: -4.9245\n",
      "Time for epoch 417 is 48.3140 sec\n",
      "Epoch 418, gen_loss: 78.1865, disc_loss: -4.3385\n",
      "Time for epoch 418 is 48.0476 sec\n",
      "Epoch 419, gen_loss: 71.5288, disc_loss: -5.1372\n",
      "Time for epoch 419 is 47.8919 sec\n",
      "Epoch 420, gen_loss: 78.7485, disc_loss: -5.1129\n",
      "Time for epoch 420 is 48.0179 sec\n",
      "Epoch 421, gen_loss: 76.5827, disc_loss: -5.1883\n",
      "Time for epoch 421 is 48.1575 sec\n",
      "Epoch 422, gen_loss: 80.4729, disc_loss: -4.9972\n",
      "Time for epoch 422 is 47.8754 sec\n",
      "Epoch 423, gen_loss: 77.6391, disc_loss: -4.9063\n",
      "Time for epoch 423 is 47.6278 sec\n",
      "Epoch 424, gen_loss: 73.0127, disc_loss: -5.1278\n",
      "Time for epoch 424 is 48.3457 sec\n",
      "Epoch 425, gen_loss: 73.9705, disc_loss: -5.3846\n",
      "Time for epoch 425 is 47.8603 sec\n",
      "Epoch 426, gen_loss: 84.2163, disc_loss: -4.8117\n",
      "Time for epoch 426 is 48.1422 sec\n",
      "Epoch 427, gen_loss: 71.1183, disc_loss: -5.2037\n",
      "Time for epoch 427 is 48.3290 sec\n",
      "Epoch 428, gen_loss: 81.6654, disc_loss: -5.0990\n",
      "Time for epoch 428 is 48.5174 sec\n",
      "Epoch 429, gen_loss: 75.7002, disc_loss: -5.1353\n",
      "Time for epoch 429 is 48.1558 sec\n",
      "Epoch 430, gen_loss: 73.3358, disc_loss: -5.0997\n",
      "Time for epoch 430 is 48.0639 sec\n",
      "Epoch 431, gen_loss: 73.4240, disc_loss: -4.8600\n",
      "Time for epoch 431 is 48.0053 sec\n",
      "Epoch 432, gen_loss: 70.1360, disc_loss: -4.8021\n",
      "Time for epoch 432 is 48.1075 sec\n",
      "Epoch 433, gen_loss: 74.1959, disc_loss: -5.3373\n",
      "Time for epoch 433 is 47.8810 sec\n",
      "Epoch 434, gen_loss: 83.6708, disc_loss: -4.9551\n",
      "Time for epoch 434 is 47.9964 sec\n",
      "Epoch 435, gen_loss: 68.7906, disc_loss: -5.4284\n",
      "Time for epoch 435 is 47.8132 sec\n",
      "Epoch 436, gen_loss: 80.3457, disc_loss: -4.4258\n",
      "Time for epoch 436 is 48.2026 sec\n",
      "Epoch 437, gen_loss: 76.9627, disc_loss: -4.9168\n",
      "Time for epoch 437 is 48.1902 sec\n",
      "Epoch 438, gen_loss: 76.8069, disc_loss: -5.0793\n",
      "Time for epoch 438 is 48.4353 sec\n",
      "Epoch 439, gen_loss: 83.0808, disc_loss: -4.7482\n",
      "Time for epoch 439 is 48.2509 sec\n",
      "Epoch 440, gen_loss: 81.4816, disc_loss: -4.7565\n",
      "Time for epoch 440 is 48.1570 sec\n",
      "Epoch 441, gen_loss: 74.8864, disc_loss: -5.4415\n",
      "Time for epoch 441 is 48.3288 sec\n",
      "Epoch 442, gen_loss: 74.4896, disc_loss: -5.3329\n",
      "Time for epoch 442 is 48.3273 sec\n",
      "Epoch 443, gen_loss: 75.1859, disc_loss: -5.2625\n",
      "Time for epoch 443 is 48.2968 sec\n",
      "Epoch 444, gen_loss: 81.2212, disc_loss: -4.8249\n",
      "Time for epoch 444 is 48.2344 sec\n",
      "Epoch 445, gen_loss: 70.5696, disc_loss: -4.9071\n",
      "Time for epoch 445 is 48.3594 sec\n",
      "Epoch 446, gen_loss: 76.6802, disc_loss: -4.9244\n",
      "Time for epoch 446 is 48.0326 sec\n",
      "Epoch 447, gen_loss: 75.9957, disc_loss: -5.1239\n",
      "Time for epoch 447 is 48.1901 sec\n",
      "Epoch 448, gen_loss: 74.1179, disc_loss: -5.0226\n",
      "Time for epoch 448 is 47.8598 sec\n",
      "Epoch 449, gen_loss: 79.2937, disc_loss: -4.9317\n",
      "Time for epoch 449 is 48.4065 sec\n",
      "Epoch 450, gen_loss: 70.1469, disc_loss: -5.2340\n",
      "Time for epoch 450 is 48.1874 sec\n",
      "Epoch 451, gen_loss: 74.9329, disc_loss: -5.1589\n",
      "Time for epoch 451 is 48.1398 sec\n",
      "Epoch 452, gen_loss: 74.1480, disc_loss: -5.3660\n",
      "Time for epoch 452 is 48.2990 sec\n",
      "Epoch 453, gen_loss: 73.0270, disc_loss: -4.8968\n",
      "Time for epoch 453 is 48.3604 sec\n",
      "Epoch 454, gen_loss: 77.4124, disc_loss: -4.9687\n",
      "Time for epoch 454 is 47.6880 sec\n",
      "Epoch 455, gen_loss: 77.7576, disc_loss: -4.8880\n",
      "Time for epoch 455 is 48.1246 sec\n",
      "Epoch 456, gen_loss: 79.0156, disc_loss: -4.8758\n",
      "Time for epoch 456 is 48.0322 sec\n",
      "Epoch 457, gen_loss: 73.9854, disc_loss: -5.0707\n",
      "Time for epoch 457 is 48.1416 sec\n",
      "Epoch 458, gen_loss: 69.4741, disc_loss: -5.1360\n",
      "Time for epoch 458 is 48.1090 sec\n",
      "Epoch 459, gen_loss: 73.4305, disc_loss: -5.3824\n",
      "Time for epoch 459 is 48.0477 sec\n",
      "Epoch 460, gen_loss: 79.3737, disc_loss: -5.0831\n",
      "Time for epoch 460 is 48.1149 sec\n",
      "Epoch 461, gen_loss: 71.4793, disc_loss: -5.3010\n",
      "Time for epoch 461 is 48.0903 sec\n",
      "Epoch 462, gen_loss: 71.9781, disc_loss: -5.2419\n",
      "Time for epoch 462 is 48.0316 sec\n",
      "Epoch 463, gen_loss: 79.0155, disc_loss: -5.1095\n",
      "Time for epoch 463 is 48.5928 sec\n",
      "Epoch 464, gen_loss: 72.3310, disc_loss: -5.1086\n",
      "Time for epoch 464 is 47.9281 sec\n",
      "Epoch 465, gen_loss: 68.4885, disc_loss: -5.4822\n",
      "Time for epoch 465 is 48.1383 sec\n",
      "Epoch 466, gen_loss: 74.2768, disc_loss: -4.9560\n",
      "Time for epoch 466 is 47.8590 sec\n",
      "Epoch 467, gen_loss: 78.4582, disc_loss: -5.2369\n",
      "Time for epoch 467 is 47.9060 sec\n",
      "Epoch 468, gen_loss: 72.2447, disc_loss: -5.2779\n",
      "Time for epoch 468 is 47.9389 sec\n",
      "Epoch 469, gen_loss: 77.0537, disc_loss: -5.2181\n",
      "Time for epoch 469 is 48.1406 sec\n",
      "Epoch 470, gen_loss: 72.4986, disc_loss: -5.2805\n",
      "Time for epoch 470 is 48.1097 sec\n",
      "Epoch 471, gen_loss: 73.3320, disc_loss: -5.1554\n",
      "Time for epoch 471 is 48.4397 sec\n",
      "Epoch 472, gen_loss: 70.6101, disc_loss: -5.6832\n",
      "Time for epoch 472 is 48.1274 sec\n",
      "Epoch 473, gen_loss: 74.3297, disc_loss: -5.4295\n",
      "Time for epoch 473 is 48.0482 sec\n",
      "Epoch 474, gen_loss: 74.3782, disc_loss: -5.4896\n",
      "Time for epoch 474 is 48.0325 sec\n",
      "Epoch 475, gen_loss: 73.8596, disc_loss: -5.4809\n",
      "Time for epoch 475 is 48.1867 sec\n",
      "Epoch 476, gen_loss: 77.6762, disc_loss: -4.9736\n",
      "Time for epoch 476 is 47.8897 sec\n",
      "Epoch 477, gen_loss: 75.1529, disc_loss: -5.1401\n",
      "Time for epoch 477 is 48.1145 sec\n",
      "Epoch 478, gen_loss: 66.0894, disc_loss: -5.4708\n",
      "Time for epoch 478 is 47.9237 sec\n",
      "Epoch 479, gen_loss: 70.1566, disc_loss: -5.2513\n",
      "Time for epoch 479 is 48.0002 sec\n",
      "Epoch 480, gen_loss: 66.6877, disc_loss: -5.7352\n",
      "Time for epoch 480 is 48.0323 sec\n",
      "Epoch 481, gen_loss: 71.4470, disc_loss: -5.0973\n",
      "Time for epoch 481 is 47.7517 sec\n",
      "Epoch 482, gen_loss: 77.6538, disc_loss: -5.1246\n",
      "Time for epoch 482 is 48.2190 sec\n",
      "Epoch 483, gen_loss: 72.9302, disc_loss: -5.3763\n",
      "Time for epoch 483 is 47.6558 sec\n",
      "Epoch 484, gen_loss: 76.5856, disc_loss: -5.2172\n",
      "Time for epoch 484 is 47.8765 sec\n",
      "Epoch 485, gen_loss: 68.4411, disc_loss: -5.2588\n",
      "Time for epoch 485 is 47.6885 sec\n",
      "Epoch 486, gen_loss: 76.3056, disc_loss: -5.0696\n",
      "Time for epoch 486 is 47.8611 sec\n",
      "Epoch 487, gen_loss: 76.8865, disc_loss: -5.2494\n",
      "Time for epoch 487 is 48.0482 sec\n",
      "Epoch 488, gen_loss: 77.0613, disc_loss: -5.3673\n",
      "Time for epoch 488 is 47.8749 sec\n",
      "Epoch 489, gen_loss: 68.1792, disc_loss: -5.5360\n",
      "Time for epoch 489 is 48.1248 sec\n",
      "Epoch 490, gen_loss: 70.5145, disc_loss: -5.1120\n",
      "Time for epoch 490 is 47.5322 sec\n",
      "Epoch 491, gen_loss: 76.2431, disc_loss: -5.4661\n",
      "Time for epoch 491 is 47.9540 sec\n",
      "Epoch 492, gen_loss: 81.8751, disc_loss: -5.3572\n",
      "Time for epoch 492 is 47.8600 sec\n",
      "Epoch 493, gen_loss: 73.9645, disc_loss: -5.2128\n",
      "Time for epoch 493 is 48.0318 sec\n",
      "Epoch 494, gen_loss: 77.1590, disc_loss: -5.1689\n",
      "Time for epoch 494 is 47.8763 sec\n",
      "Epoch 495, gen_loss: 74.6428, disc_loss: -5.2910\n",
      "Time for epoch 495 is 47.9371 sec\n",
      "Epoch 496, gen_loss: 73.0704, disc_loss: -5.3473\n",
      "Time for epoch 496 is 48.1561 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497, gen_loss: 72.4731, disc_loss: -5.5860\n",
      "Time for epoch 497 is 47.8750 sec\n",
      "Epoch 498, gen_loss: 79.0859, disc_loss: -5.3675\n",
      "Time for epoch 498 is 48.2337 sec\n",
      "Epoch 499, gen_loss: 72.2545, disc_loss: -5.4571\n",
      "Time for epoch 499 is 48.1886 sec\n",
      "Epoch 500, gen_loss: 70.9882, disc_loss: -4.9813\n",
      "Time for epoch 500 is 48.0646 sec\n",
      "Epoch 501, gen_loss: 74.2279, disc_loss: -5.1536\n",
      "Time for epoch 501 is 47.9545 sec\n",
      "Epoch 502, gen_loss: 69.4165, disc_loss: -5.4914\n",
      "Time for epoch 502 is 48.0316 sec\n",
      "Epoch 503, gen_loss: 69.2687, disc_loss: -5.5149\n",
      "Time for epoch 503 is 47.9382 sec\n",
      "Epoch 504, gen_loss: 71.8628, disc_loss: -5.4613\n",
      "Time for epoch 504 is 48.2510 sec\n",
      "Epoch 505, gen_loss: 75.7049, disc_loss: -5.1663\n",
      "Time for epoch 505 is 48.2339 sec\n",
      "Epoch 506, gen_loss: 77.8928, disc_loss: -5.1555\n",
      "Time for epoch 506 is 47.9225 sec\n",
      "Epoch 507, gen_loss: 75.7896, disc_loss: -5.5916\n",
      "Time for epoch 507 is 48.0494 sec\n",
      "Epoch 508, gen_loss: 74.9821, disc_loss: -5.3869\n",
      "Time for epoch 508 is 47.0019 sec\n",
      "Epoch 509, gen_loss: 76.3048, disc_loss: -5.4775\n",
      "Time for epoch 509 is 47.8141 sec\n",
      "Epoch 510, gen_loss: 68.5187, disc_loss: -5.1321\n",
      "Time for epoch 510 is 47.8768 sec\n",
      "Epoch 511, gen_loss: 75.6411, disc_loss: -5.1926\n",
      "Time for epoch 511 is 48.4602 sec\n",
      "Epoch 512, gen_loss: 70.8353, disc_loss: -5.5625\n",
      "Time for epoch 512 is 47.9606 sec\n",
      "Epoch 513, gen_loss: 63.3350, disc_loss: -5.5245\n",
      "Time for epoch 513 is 47.9378 sec\n",
      "Epoch 514, gen_loss: 66.1815, disc_loss: -5.5767\n",
      "Time for epoch 514 is 48.0931 sec\n",
      "Epoch 515, gen_loss: 71.7108, disc_loss: -5.3386\n",
      "Time for epoch 515 is 48.4378 sec\n",
      "Epoch 516, gen_loss: 69.2412, disc_loss: -5.4194\n",
      "Time for epoch 516 is 48.1902 sec\n",
      "Epoch 517, gen_loss: 78.9822, disc_loss: -5.2373\n",
      "Time for epoch 517 is 48.1896 sec\n",
      "Epoch 518, gen_loss: 68.9094, disc_loss: -5.6627\n",
      "Time for epoch 518 is 48.3900 sec\n",
      "Epoch 519, gen_loss: 68.3486, disc_loss: -5.4677\n",
      "Time for epoch 519 is 48.5634 sec\n",
      "Epoch 520, gen_loss: 70.1259, disc_loss: -5.1751\n",
      "Time for epoch 520 is 48.3918 sec\n",
      "Epoch 521, gen_loss: 65.8242, disc_loss: -5.5016\n",
      "Time for epoch 521 is 48.4232 sec\n",
      "Epoch 522, gen_loss: 68.6282, disc_loss: -5.3552\n",
      "Time for epoch 522 is 48.8465 sec\n",
      "Epoch 523, gen_loss: 68.4868, disc_loss: -5.7519\n",
      "Time for epoch 523 is 49.5324 sec\n",
      "Epoch 524, gen_loss: 70.2766, disc_loss: -5.4351\n",
      "Time for epoch 524 is 48.8301 sec\n",
      "Epoch 525, gen_loss: 71.6935, disc_loss: -5.3685\n",
      "Time for epoch 525 is 48.5485 sec\n",
      "Epoch 526, gen_loss: 72.3128, disc_loss: -5.5556\n",
      "Time for epoch 526 is 48.9223 sec\n",
      "Epoch 527, gen_loss: 68.9070, disc_loss: -5.4659\n",
      "Time for epoch 527 is 49.0573 sec\n",
      "Epoch 528, gen_loss: 79.3326, disc_loss: -5.0544\n",
      "Time for epoch 528 is 48.9539 sec\n",
      "Epoch 529, gen_loss: 74.2222, disc_loss: -5.2306\n",
      "Time for epoch 529 is 49.5173 sec\n",
      "Epoch 530, gen_loss: 71.3311, disc_loss: -5.4466\n",
      "Time for epoch 530 is 48.9980 sec\n",
      "Epoch 531, gen_loss: 75.9215, disc_loss: -4.9532\n",
      "Time for epoch 531 is 49.4383 sec\n",
      "Epoch 532, gen_loss: 67.7408, disc_loss: -5.3516\n",
      "Time for epoch 532 is 49.2813 sec\n",
      "Epoch 533, gen_loss: 72.3789, disc_loss: -5.4179\n",
      "Time for epoch 533 is 49.0716 sec\n",
      "Epoch 534, gen_loss: 79.2936, disc_loss: -5.1720\n",
      "Time for epoch 534 is 49.4771 sec\n",
      "Epoch 535, gen_loss: 67.9988, disc_loss: -5.7454\n",
      "Time for epoch 535 is 49.5790 sec\n",
      "Epoch 536, gen_loss: 68.1907, disc_loss: -5.4353\n",
      "Time for epoch 536 is 49.4872 sec\n",
      "Epoch 537, gen_loss: 71.2511, disc_loss: -5.2831\n",
      "Time for epoch 537 is 49.6092 sec\n",
      "Epoch 538, gen_loss: 66.5994, disc_loss: -5.8196\n",
      "Time for epoch 538 is 49.7680 sec\n",
      "Epoch 539, gen_loss: 72.5590, disc_loss: -5.5032\n",
      "Time for epoch 539 is 49.9845 sec\n",
      "Epoch 540, gen_loss: 68.4630, disc_loss: -5.4921\n",
      "Time for epoch 540 is 49.5172 sec\n",
      "Epoch 541, gen_loss: 72.1721, disc_loss: -5.5158\n",
      "Time for epoch 541 is 49.6411 sec\n",
      "Epoch 542, gen_loss: 71.9344, disc_loss: -5.5190\n",
      "Time for epoch 542 is 49.6267 sec\n",
      "Epoch 543, gen_loss: 71.1177, disc_loss: -5.3162\n",
      "Time for epoch 543 is 49.8921 sec\n",
      "Epoch 544, gen_loss: 65.2032, disc_loss: -5.6623\n",
      "Time for epoch 544 is 50.2511 sec\n",
      "Epoch 545, gen_loss: 73.6563, disc_loss: -5.1695\n",
      "Time for epoch 545 is 63.0006 sec\n",
      "Epoch 546, gen_loss: 70.6480, disc_loss: -5.1132\n",
      "Time for epoch 546 is 48.4575 sec\n",
      "Epoch 547, gen_loss: 71.3287, disc_loss: -5.5853\n",
      "Time for epoch 547 is 47.1890 sec\n",
      "Epoch 548, gen_loss: 64.2337, disc_loss: -5.7451\n",
      "Time for epoch 548 is 47.3908 sec\n",
      "Epoch 549, gen_loss: 72.5008, disc_loss: -5.2692\n",
      "Time for epoch 549 is 50.2200 sec\n",
      "Epoch 550, gen_loss: 70.2458, disc_loss: -5.3605\n",
      "Time for epoch 550 is 50.1419 sec\n",
      "Epoch 551, gen_loss: 68.4114, disc_loss: -5.6669\n",
      "Time for epoch 551 is 49.9316 sec\n",
      "Epoch 552, gen_loss: 64.7464, disc_loss: -5.3651\n",
      "Time for epoch 552 is 49.8311 sec\n",
      "Epoch 553, gen_loss: 66.7710, disc_loss: -5.7377\n",
      "Time for epoch 553 is 49.8734 sec\n",
      "Epoch 554, gen_loss: 68.3902, disc_loss: -5.4404\n",
      "Time for epoch 554 is 49.6096 sec\n",
      "Epoch 555, gen_loss: 73.3446, disc_loss: -5.4558\n",
      "Time for epoch 555 is 49.6280 sec\n",
      "Epoch 556, gen_loss: 68.3993, disc_loss: -5.4970\n",
      "Time for epoch 556 is 50.1093 sec\n",
      "Epoch 557, gen_loss: 72.3585, disc_loss: -5.6846\n",
      "Time for epoch 557 is 50.0777 sec\n",
      "Epoch 558, gen_loss: 73.3206, disc_loss: -5.7585\n",
      "Time for epoch 558 is 50.0471 sec\n",
      "Epoch 559, gen_loss: 67.6421, disc_loss: -5.3268\n",
      "Time for epoch 559 is 50.0020 sec\n",
      "Epoch 560, gen_loss: 65.5205, disc_loss: -5.8783\n",
      "Time for epoch 560 is 49.6554 sec\n",
      "Epoch 561, gen_loss: 66.1638, disc_loss: -5.6321\n",
      "Time for epoch 561 is 49.4358 sec\n",
      "Epoch 562, gen_loss: 65.1233, disc_loss: -5.5829\n",
      "Time for epoch 562 is 49.5622 sec\n",
      "Epoch 563, gen_loss: 65.6476, disc_loss: -5.8036\n",
      "Time for epoch 563 is 49.7040 sec\n",
      "Epoch 564, gen_loss: 71.1579, disc_loss: -5.5779\n",
      "Time for epoch 564 is 49.8910 sec\n",
      "Epoch 565, gen_loss: 65.2130, disc_loss: -5.8880\n",
      "Time for epoch 565 is 49.8521 sec\n",
      "Epoch 566, gen_loss: 68.2826, disc_loss: -5.6078\n",
      "Time for epoch 566 is 49.8919 sec\n",
      "Epoch 567, gen_loss: 70.8857, disc_loss: -5.4175\n",
      "Time for epoch 567 is 49.7201 sec\n",
      "Epoch 568, gen_loss: 73.0506, disc_loss: -5.2077\n",
      "Time for epoch 568 is 49.8923 sec\n",
      "Epoch 569, gen_loss: 70.1757, disc_loss: -5.2659\n",
      "Time for epoch 569 is 49.7980 sec\n",
      "Epoch 570, gen_loss: 66.9559, disc_loss: -5.6375\n",
      "Time for epoch 570 is 50.0164 sec\n",
      "Epoch 571, gen_loss: 67.0675, disc_loss: -5.5342\n",
      "Time for epoch 571 is 49.9070 sec\n",
      "Epoch 572, gen_loss: 70.4876, disc_loss: -5.2792\n",
      "Time for epoch 572 is 49.9691 sec\n",
      "Epoch 573, gen_loss: 69.1814, disc_loss: -5.5058\n",
      "Time for epoch 573 is 50.2195 sec\n",
      "Epoch 574, gen_loss: 67.3025, disc_loss: -5.4733\n",
      "Time for epoch 574 is 50.4394 sec\n",
      "Epoch 575, gen_loss: 67.0318, disc_loss: -5.5109\n",
      "Time for epoch 575 is 50.1883 sec\n",
      "Epoch 576, gen_loss: 69.2545, disc_loss: -5.5997\n",
      "Time for epoch 576 is 50.2202 sec\n",
      "Epoch 577, gen_loss: 70.9454, disc_loss: -5.7035\n",
      "Time for epoch 577 is 50.0347 sec\n",
      "Epoch 578, gen_loss: 69.3219, disc_loss: -5.8609\n",
      "Time for epoch 578 is 50.1168 sec\n",
      "Epoch 579, gen_loss: 68.3906, disc_loss: -5.9118\n",
      "Time for epoch 579 is 50.3438 sec\n",
      "Epoch 580, gen_loss: 66.2745, disc_loss: -5.5525\n",
      "Time for epoch 580 is 50.1416 sec\n",
      "Epoch 581, gen_loss: 68.1837, disc_loss: -5.8123\n",
      "Time for epoch 581 is 49.9071 sec\n",
      "Epoch 582, gen_loss: 73.1898, disc_loss: -5.2841\n",
      "Time for epoch 582 is 50.0469 sec\n",
      "Epoch 583, gen_loss: 62.5538, disc_loss: -5.6832\n",
      "Time for epoch 583 is 49.7365 sec\n",
      "Epoch 584, gen_loss: 74.4654, disc_loss: -5.3400\n",
      "Time for epoch 584 is 49.9376 sec\n",
      "Epoch 585, gen_loss: 66.9277, disc_loss: -5.4919\n",
      "Time for epoch 585 is 50.2200 sec\n",
      "Epoch 586, gen_loss: 65.9704, disc_loss: -5.8773\n",
      "Time for epoch 586 is 50.1879 sec\n",
      "Epoch 587, gen_loss: 73.4740, disc_loss: -5.2332\n",
      "Time for epoch 587 is 50.3608 sec\n",
      "Epoch 588, gen_loss: 67.1272, disc_loss: -5.4916\n",
      "Time for epoch 588 is 49.9707 sec\n",
      "Epoch 589, gen_loss: 67.1687, disc_loss: -5.7194\n",
      "Time for epoch 589 is 50.2357 sec\n",
      "Epoch 590, gen_loss: 73.3018, disc_loss: -4.5897\n",
      "Time for epoch 590 is 49.8468 sec\n",
      "Epoch 591, gen_loss: 67.4926, disc_loss: -5.4344\n",
      "Time for epoch 591 is 50.0321 sec\n",
      "Epoch 592, gen_loss: 65.3629, disc_loss: -6.0077\n",
      "Time for epoch 592 is 50.4215 sec\n",
      "Epoch 593, gen_loss: 65.7311, disc_loss: -6.0198\n",
      "Time for epoch 593 is 50.5145 sec\n",
      "Epoch 594, gen_loss: 77.6246, disc_loss: -4.9309\n",
      "Time for epoch 594 is 50.6052 sec\n",
      "Epoch 595, gen_loss: 66.5914, disc_loss: -5.4836\n",
      "Time for epoch 595 is 50.5877 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596, gen_loss: 71.8270, disc_loss: -5.6767\n",
      "Time for epoch 596 is 50.1571 sec\n",
      "Epoch 597, gen_loss: 67.2970, disc_loss: -5.9304\n",
      "Time for epoch 597 is 50.1260 sec\n",
      "Epoch 598, gen_loss: 76.0542, disc_loss: -5.2944\n",
      "Time for epoch 598 is 50.6576 sec\n",
      "Epoch 599, gen_loss: 69.9120, disc_loss: -5.4837\n",
      "Time for epoch 599 is 50.2825 sec\n",
      "Epoch 600, gen_loss: 63.6009, disc_loss: -5.5662\n",
      "Time for epoch 600 is 50.3137 sec\n",
      "Epoch 601, gen_loss: 67.9142, disc_loss: -5.6763\n",
      "Time for epoch 601 is 50.4698 sec\n",
      "Epoch 602, gen_loss: 69.1812, disc_loss: -5.5412\n",
      "Time for epoch 602 is 50.2685 sec\n",
      "Epoch 603, gen_loss: 69.0766, disc_loss: -5.5512\n",
      "Time for epoch 603 is 50.4852 sec\n",
      "Epoch 604, gen_loss: 73.5375, disc_loss: -5.2789\n",
      "Time for epoch 604 is 50.6726 sec\n",
      "Epoch 605, gen_loss: 62.7279, disc_loss: -5.6614\n",
      "Time for epoch 605 is 50.5332 sec\n",
      "Epoch 606, gen_loss: 68.7714, disc_loss: -5.5942\n",
      "Time for epoch 606 is 50.5318 sec\n",
      "Epoch 607, gen_loss: 70.7850, disc_loss: -5.6094\n",
      "Time for epoch 607 is 50.5171 sec\n",
      "Epoch 608, gen_loss: 69.3050, disc_loss: -5.6838\n",
      "Time for epoch 608 is 50.1869 sec\n",
      "Epoch 609, gen_loss: 74.7766, disc_loss: -5.6009\n",
      "Time for epoch 609 is 50.6137 sec\n",
      "Epoch 610, gen_loss: 73.2885, disc_loss: -5.4791\n",
      "Time for epoch 610 is 50.0640 sec\n",
      "Epoch 611, gen_loss: 68.9223, disc_loss: -5.0717\n",
      "Time for epoch 611 is 50.5010 sec\n",
      "Epoch 612, gen_loss: 70.2277, disc_loss: -5.6812\n",
      "Time for epoch 612 is 50.3438 sec\n",
      "Epoch 613, gen_loss: 78.3521, disc_loss: -5.5244\n",
      "Time for epoch 613 is 50.5333 sec\n",
      "Epoch 614, gen_loss: 70.0075, disc_loss: -5.7331\n",
      "Time for epoch 614 is 50.5627 sec\n",
      "Epoch 615, gen_loss: 69.4388, disc_loss: -5.7430\n",
      "Time for epoch 615 is 50.2848 sec\n",
      "Epoch 616, gen_loss: 63.2455, disc_loss: -5.8425\n",
      "Time for epoch 616 is 50.6625 sec\n",
      "Epoch 617, gen_loss: 72.6526, disc_loss: -5.5368\n",
      "Time for epoch 617 is 50.2362 sec\n",
      "Epoch 618, gen_loss: 68.4414, disc_loss: -5.7245\n",
      "Time for epoch 618 is 50.6437 sec\n",
      "Epoch 619, gen_loss: 67.8368, disc_loss: -5.5917\n",
      "Time for epoch 619 is 50.5483 sec\n",
      "Epoch 620, gen_loss: 68.4152, disc_loss: -5.9864\n",
      "Time for epoch 620 is 50.5016 sec\n",
      "Epoch 621, gen_loss: 69.2180, disc_loss: -5.7671\n",
      "Time for epoch 621 is 50.2013 sec\n",
      "Epoch 622, gen_loss: 76.0885, disc_loss: -5.4212\n",
      "Time for epoch 622 is 49.8449 sec\n",
      "Epoch 623, gen_loss: 61.0967, disc_loss: -5.8745\n",
      "Time for epoch 623 is 50.6572 sec\n",
      "Epoch 624, gen_loss: 68.5563, disc_loss: -5.3307\n",
      "Time for epoch 624 is 50.7512 sec\n",
      "Epoch 625, gen_loss: 67.6609, disc_loss: -5.8040\n",
      "Time for epoch 625 is 50.4848 sec\n",
      "Epoch 626, gen_loss: 65.9555, disc_loss: -5.9238\n",
      "Time for epoch 626 is 50.2198 sec\n",
      "Epoch 627, gen_loss: 71.1681, disc_loss: -5.4687\n",
      "Time for epoch 627 is 50.0002 sec\n",
      "Epoch 628, gen_loss: 69.0553, disc_loss: -5.8055\n",
      "Time for epoch 628 is 50.5342 sec\n",
      "Epoch 629, gen_loss: 66.6812, disc_loss: -5.8675\n",
      "Time for epoch 629 is 50.3319 sec\n",
      "Epoch 630, gen_loss: 68.8971, disc_loss: -5.6363\n",
      "Time for epoch 630 is 50.4273 sec\n",
      "Epoch 631, gen_loss: 69.3262, disc_loss: -5.4676\n",
      "Time for epoch 631 is 50.4682 sec\n",
      "Epoch 632, gen_loss: 76.7783, disc_loss: -5.5767\n",
      "Time for epoch 632 is 50.3776 sec\n",
      "Epoch 633, gen_loss: 62.5718, disc_loss: -5.8930\n",
      "Time for epoch 633 is 50.3896 sec\n",
      "Epoch 634, gen_loss: 67.8587, disc_loss: -5.5730\n",
      "Time for epoch 634 is 50.5950 sec\n",
      "Epoch 635, gen_loss: 66.5558, disc_loss: -5.3980\n",
      "Time for epoch 635 is 50.8609 sec\n",
      "Epoch 636, gen_loss: 68.3563, disc_loss: -5.7113\n",
      "Time for epoch 636 is 50.6099 sec\n",
      "Epoch 637, gen_loss: 63.3673, disc_loss: -6.0123\n",
      "Time for epoch 637 is 50.4064 sec\n",
      "Epoch 638, gen_loss: 69.0789, disc_loss: -5.6321\n",
      "Time for epoch 638 is 50.5188 sec\n",
      "Epoch 639, gen_loss: 67.8585, disc_loss: -5.7916\n",
      "Time for epoch 639 is 50.3742 sec\n",
      "Epoch 640, gen_loss: 73.1089, disc_loss: -5.2105\n",
      "Time for epoch 640 is 50.4378 sec\n",
      "Epoch 641, gen_loss: 67.7646, disc_loss: -5.8025\n",
      "Time for epoch 641 is 50.4852 sec\n",
      "Epoch 642, gen_loss: 69.5349, disc_loss: -5.5581\n",
      "Time for epoch 642 is 50.7800 sec\n",
      "Epoch 643, gen_loss: 63.4791, disc_loss: -5.6322\n",
      "Time for epoch 643 is 50.3858 sec\n",
      "Epoch 644, gen_loss: 67.9040, disc_loss: -5.9535\n",
      "Time for epoch 644 is 50.6246 sec\n",
      "Epoch 645, gen_loss: 66.1856, disc_loss: -5.9433\n",
      "Time for epoch 645 is 50.4845 sec\n",
      "Epoch 646, gen_loss: 68.5939, disc_loss: -5.6430\n",
      "Time for epoch 646 is 50.9060 sec\n",
      "Epoch 647, gen_loss: 60.4882, disc_loss: -5.9604\n",
      "Time for epoch 647 is 50.7050 sec\n",
      "Epoch 648, gen_loss: 63.0911, disc_loss: -5.9381\n",
      "Time for epoch 648 is 50.8282 sec\n",
      "Epoch 649, gen_loss: 64.4125, disc_loss: -6.2303\n",
      "Time for epoch 649 is 49.5283 sec\n",
      "Epoch 650, gen_loss: 62.5768, disc_loss: -5.9461\n",
      "Time for epoch 650 is 47.9172 sec\n",
      "Epoch 651, gen_loss: 67.4303, disc_loss: -5.9834\n",
      "Time for epoch 651 is 47.7206 sec\n",
      "Epoch 652, gen_loss: 63.2761, disc_loss: -6.0565\n",
      "Time for epoch 652 is 47.5850 sec\n",
      "Epoch 653, gen_loss: 68.4615, disc_loss: -5.8365\n",
      "Time for epoch 653 is 48.1211 sec\n",
      "Epoch 654, gen_loss: 66.4830, disc_loss: -5.7055\n",
      "Time for epoch 654 is 48.2483 sec\n",
      "Epoch 655, gen_loss: 69.7619, disc_loss: -5.7424\n",
      "Time for epoch 655 is 47.6130 sec\n",
      "Epoch 656, gen_loss: 64.8285, disc_loss: -5.8559\n",
      "Time for epoch 656 is 47.9756 sec\n",
      "Epoch 657, gen_loss: 68.4295, disc_loss: -5.9502\n",
      "Time for epoch 657 is 47.5300 sec\n",
      "Epoch 658, gen_loss: 68.6011, disc_loss: -5.8329\n",
      "Time for epoch 658 is 47.1074 sec\n",
      "Epoch 659, gen_loss: 69.4585, disc_loss: -5.8938\n",
      "Time for epoch 659 is 47.8448 sec\n",
      "Epoch 660, gen_loss: 61.9934, disc_loss: -6.0776\n",
      "Time for epoch 660 is 47.6766 sec\n",
      "Epoch 661, gen_loss: 60.3079, disc_loss: -6.1303\n",
      "Time for epoch 661 is 47.0119 sec\n",
      "Epoch 662, gen_loss: 66.8401, disc_loss: -5.8679\n",
      "Time for epoch 662 is 47.9456 sec\n",
      "Epoch 663, gen_loss: 64.2610, disc_loss: -6.0852\n",
      "Time for epoch 663 is 47.4263 sec\n",
      "Epoch 664, gen_loss: 67.6367, disc_loss: -5.8062\n",
      "Time for epoch 664 is 47.8375 sec\n",
      "Epoch 665, gen_loss: 74.7510, disc_loss: -5.6028\n",
      "Time for epoch 665 is 47.7375 sec\n",
      "Epoch 666, gen_loss: 60.2225, disc_loss: -6.2680\n",
      "Time for epoch 666 is 47.8139 sec\n",
      "Epoch 667, gen_loss: 66.7734, disc_loss: -5.8337\n",
      "Time for epoch 667 is 47.9405 sec\n",
      "Epoch 668, gen_loss: 69.6796, disc_loss: -5.8238\n",
      "Time for epoch 668 is 47.2563 sec\n",
      "Epoch 669, gen_loss: 68.4897, disc_loss: -5.8720\n",
      "Time for epoch 669 is 47.4129 sec\n",
      "Epoch 670, gen_loss: 68.6894, disc_loss: -6.0315\n",
      "Time for epoch 670 is 47.3176 sec\n",
      "Epoch 671, gen_loss: 64.3934, disc_loss: -6.0860\n",
      "Time for epoch 671 is 47.4704 sec\n",
      "Epoch 672, gen_loss: 61.8152, disc_loss: -6.2725\n",
      "Time for epoch 672 is 47.7869 sec\n",
      "Epoch 673, gen_loss: 61.1307, disc_loss: -6.1706\n",
      "Time for epoch 673 is 47.6938 sec\n",
      "Epoch 674, gen_loss: 66.8698, disc_loss: -5.8581\n",
      "Time for epoch 674 is 47.8305 sec\n",
      "Epoch 675, gen_loss: 64.3228, disc_loss: -5.8505\n",
      "Time for epoch 675 is 47.7654 sec\n",
      "Epoch 676, gen_loss: 63.4084, disc_loss: -5.9598\n",
      "Time for epoch 676 is 47.6591 sec\n",
      "Epoch 677, gen_loss: 65.0317, disc_loss: -6.0377\n",
      "Time for epoch 677 is 48.1728 sec\n",
      "Epoch 678, gen_loss: 61.6969, disc_loss: -5.9944\n",
      "Time for epoch 678 is 47.6619 sec\n",
      "Epoch 679, gen_loss: 63.9474, disc_loss: -5.7753\n",
      "Time for epoch 679 is 47.8610 sec\n",
      "Epoch 680, gen_loss: 64.6185, disc_loss: -5.8965\n",
      "Time for epoch 680 is 47.2219 sec\n",
      "Epoch 681, gen_loss: 66.5952, disc_loss: -5.7800\n",
      "Time for epoch 681 is 47.5457 sec\n",
      "Epoch 682, gen_loss: 66.9220, disc_loss: -5.7396\n",
      "Time for epoch 682 is 47.4313 sec\n",
      "Epoch 683, gen_loss: 67.5324, disc_loss: -5.4410\n",
      "Time for epoch 683 is 47.6680 sec\n",
      "Epoch 684, gen_loss: 65.5378, disc_loss: -6.0843\n",
      "Time for epoch 684 is 47.5664 sec\n",
      "Epoch 685, gen_loss: 68.2536, disc_loss: -5.9368\n",
      "Time for epoch 685 is 47.9194 sec\n",
      "Epoch 686, gen_loss: 65.2827, disc_loss: -5.5891\n",
      "Time for epoch 686 is 47.7590 sec\n",
      "Epoch 687, gen_loss: 65.2842, disc_loss: -6.3084\n",
      "Time for epoch 687 is 47.6034 sec\n",
      "Epoch 688, gen_loss: 65.5057, disc_loss: -6.0698\n",
      "Time for epoch 688 is 47.8206 sec\n",
      "Epoch 689, gen_loss: 62.6566, disc_loss: -6.0183\n",
      "Time for epoch 689 is 47.5706 sec\n",
      "Epoch 690, gen_loss: 64.5110, disc_loss: -6.1642\n",
      "Time for epoch 690 is 47.6262 sec\n",
      "Epoch 691, gen_loss: 70.1805, disc_loss: -5.8475\n",
      "Time for epoch 691 is 47.6263 sec\n",
      "Epoch 692, gen_loss: 63.2070, disc_loss: -5.9488\n",
      "Time for epoch 692 is 47.6169 sec\n",
      "Epoch 693, gen_loss: 67.4348, disc_loss: -6.0635\n",
      "Time for epoch 693 is 47.1899 sec\n",
      "Epoch 694, gen_loss: 67.6380, disc_loss: -5.6827\n",
      "Time for epoch 694 is 47.9418 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 695, gen_loss: 65.1904, disc_loss: -5.8283\n",
      "Time for epoch 695 is 47.8145 sec\n",
      "Epoch 696, gen_loss: 60.9519, disc_loss: -6.0643\n",
      "Time for epoch 696 is 47.9565 sec\n",
      "Epoch 697, gen_loss: 62.6190, disc_loss: -6.0912\n",
      "Time for epoch 697 is 47.3671 sec\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e775735",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "這次的competition，我們做了許多的嘗試，雖然最後的排名沒有很高，卻讓我們學到很多，也有很多可以討論的點。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b760b",
   "metadata": {},
   "source": [
    "### improved WGAN without \"3rd pair and BERT\"\n",
    "在最一開始的嘗試中，我們是想要把助教給的template中，loss計算的部分改成improved WGAN，\n",
    "但老師有說過，必須要加入第三個pair，強迫模型學習text，否則generator會和discriminator串通好兩個都不學text。\n",
    "\n",
    "而我們就是沒有加入第三個pair的計算，因此就遇到了這個問題。\n",
    "下圖是我們train出來的結果，可以看到其實圖片本身是呈現得還不錯的，花朵的形狀也不會有模糊，\n",
    "但問題就是，理論上同一個row應該都是對同一段text產生圖片，但我們產生出來的圖片，同一個row之間卻都差超多，\n",
    "這代表我們都沒有學到text，只有學到如何generate出花朵的圖片。\n",
    "\n",
    "不過，這樣學出來的成果丟去評分，竟然是我們最高分的成果，這讓我們很疑惑，或許花朵的清晰度跟解析度也很重要吧。\n",
    "\n",
    "<img src=\"319210186_902609240731653_2122270037341232288_n.png\" width=40% height=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538d9aa",
   "metadata": {},
   "source": [
    "### improved WGAN with \"3rd pair and BERT\"\n",
    "之後，我們加入了BERT，也修改了loss的算法，加入了第三個pair。\n",
    "將不同epoch的圖片呈現如下，可以發現這次就真的有學到text了。\n",
    "\n",
    "不過，最終分數卻沒有比未優化的版本高，我想是因為我們train的epoch還不夠多，所以呈現出來的圖片品質還不夠好。\n",
    "\n",
    "\n",
    "**epoch:100**\n",
    "<img src=\"320390088_719045642712569_6553334425431985761_n.png\" width=40% height=40%>\n",
    "\n",
    "**epoch:400**\n",
    "<img src=\"319066916_1160095808207139_3818734317308985616_n.png\" width=40% height=40%>\n",
    "\n",
    "**epoch:800**\n",
    "<img src=\"319909598_465040865803085_1750117505661270291_n.png\" width=40% height=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
