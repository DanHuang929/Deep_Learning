{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# team20 人工智障\n",
    "108062127 林湛悅\n",
    "108062313 黃允暘\n",
    "108062140 陳昇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did you preprocess data, e.g. cleaning, feature engineering, etc?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    We use the beautifulsoup to find the html tag of the text and select feature, convert to lowercase and append all emoticons behind. At first we choose the  title/topic/channel/week/time/textlen/imagenum/sentiment to be our feature. In the training process we find there are some feature useless, so we remove the useless feature.\n",
    "    We keep topic/weekday/time/textlen/sentiment as our feature.  For weekday we use the one hot to seperate the weekday. And for sentiment we use the SentimentIntensityAnalyzer to vectorize the text.  \n",
    "    We use the topic for X_train, X_test, use hstack to concate. Do stop-word removal then use the CountVectorizer to build bag of word model to vectorize. Finally concate others feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did you build the classifier, e.g. model, training algorithm, special techniques, etc?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    We use LGBMClassifier as our classifier, and do 50 rounds of training. Each round split the X_tain_vectorized to train data and validation data. If it can get better score, we use that round model to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions, including interesting findings, pitfalls, takeaway lessons, etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "     In the beggining, we use too many feature to predict, and we find too many feature also influence the accuracy. So we delete some feature and get improvement. We try a lot to select useful feature, like choose the key word of the feature. However it get worse, we think the only one key word don't have good information, it still need to consider the whole article. And the different preprocess have big difference. For example, weekday do one hot can have good predict. And the classifier also important. We try many classifier, finally choose the classifier have best accuracy in our traing. In the competetion, we know data preprocess is important can't be ignore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
