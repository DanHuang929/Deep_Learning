{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab11461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b4cfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c7903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3340\n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    \n",
    "    return text\n",
    "print(len(preprocessor(df.loc[0,'Page content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91698ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_title(text):\n",
    "    # remove HTML tags\n",
    "    titles = BeautifulSoup(text, 'html.parser')\n",
    "    titles = titles.find_all('h1', class_='title')\n",
    "    t=\"\"\n",
    "    for title in titles:\n",
    "        t+=str(title.getText())\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, t)\n",
    "    t = re.sub(r, '', t)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    t = re.sub('[\\W]+', ' ', t.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48021e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentiment(X):\n",
    "    ret=[]\n",
    "    for text in X:\n",
    "        # remove HTML tags\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "        # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "        r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "        emoticons = re.findall(r, text)\n",
    "        text = re.sub(r, '', text)\n",
    "\n",
    "        # convert to lowercase and append all emoticons behind (with space in between)\n",
    "        # replace('-','') removes nose of emoticons\n",
    "        text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "\n",
    "       \n",
    "\n",
    "        scores = sid.polarity_scores(text)\n",
    "\n",
    "\n",
    "        temp=[]\n",
    "        for key, value in scores.items():\n",
    "            temp.append(value)\n",
    "            \n",
    "        ret.append(temp)\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544c6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "# print(tokenizer_stem_nostop(preprocessor(df.loc[0,'Page content'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16cdf9",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc88c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "# print(next(get_stream(path='train_2.csv', size=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e913d6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hashvec = HashingVectorizer(n_features=2**12, \n",
    "#                             preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# # loss='log' gives logistic regression\n",
    "# clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# # clf = PassiveAggressiveClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# batch_size = 1000\n",
    "# # stream = get_stream(path='train_2.csv', size=batch_size)\n",
    "# classes = np.array([0, 1])\n",
    "# train_auc, val_auc = [], []\n",
    "# # we use one batch for training and another for validation in each iteration\n",
    "# iters = int((26000+batch_size-1)/(batch_size))\n",
    "\n",
    "# stream = get_stream(path='train_2.csv', size=batch_size)\n",
    "# for i in range(iters):\n",
    "#     if(i<iters*0.7):\n",
    "#         batch = next(stream)\n",
    "#         X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "#         if X_train is None:\n",
    "#             break\n",
    "#         X_train = hashvec.transform(X_train)\n",
    "#         clf.partial_fit(X_train, y_train, classes=classes)\n",
    "#         score = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
    "#         train_auc.append(score)\n",
    "#         print('train: [{}/{}] {}'.format((i+1)*(batch_size), 26000, score))\n",
    "\n",
    "#     # validate\n",
    "#     else:\n",
    "#         batch = next(stream)\n",
    "#         X_val, y_val = batch['Page content'], batch['Popularity']\n",
    "#         score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "#         val_auc.append(score)\n",
    "#         print('valid: [{}/{}] {}'.format((i+1)*(batch_size), 26000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb05c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] 0.4786392531744888\n",
      "[4000/25000] 0.5456586136264335\n",
      "[6000/25000] 0.5065220260881043\n",
      "[8000/25000] 0.5283563332479508\n",
      "[10000/25000] 0.5124952112108075\n",
      "[12000/25000] 0.4888575988735594\n",
      "[14000/25000] 0.4982454315198125\n",
      "[16000/25000] 0.5280550099803594\n",
      "[18000/25000] 0.5088277302351261\n",
      "[20000/25000] 0.5140518197398931\n",
      "[22000/25000] 0.5236976498463115\n",
      "[24000/25000] 0.5434806956911311\n"
     ]
    }
   ],
   "source": [
    "# hashvec_title = HashingVectorizer(n_features=2**9, \n",
    "#                             preprocessor=preprocessor_title, tokenizer=tokenizer_stem_nostop)\n",
    "# hashvec = HashingVectorizer(n_features=2**20, \n",
    "#                             preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# # loss='log' gives logistic regression\n",
    "# clf = SGDClassifier(loss='log', tol=1e-3)\n",
    "# batch_size = 1000\n",
    "# stream = get_stream(path='train_2.csv', size=batch_size)\n",
    "# classes = np.array([0, 1])\n",
    "# train_auc, val_auc = [], []\n",
    "# # we use one batch for training and another for validation in each iteration\n",
    "# iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "# for i in range(iters):\n",
    "#     batch = next(stream)\n",
    "#     X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "#     if X_train is None:\n",
    "#         break\n",
    "        \n",
    "#     X_train_normal = hashvec.transform(X_train)\n",
    "#     X_train_title = hashvec_title.transform(X_train)\n",
    "#     X_train_transformed = np.hstack((X_train_normal.toarray(), X_train_title.toarray()))\n",
    "    \n",
    "    \n",
    "#     clf.partial_fit(X_train_transformed, y_train, classes=classes)\n",
    "#     train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train_transformed)[:,1]))\n",
    "    \n",
    "#     # validate\n",
    "#     batch = next(stream)\n",
    "#     X_val, y_val = batch['Page content'], batch['Popularity']\n",
    "    \n",
    "#     X_val_normal = hashvec.transform(X_val)\n",
    "#     X_val_title = hashvec_title.transform(X_val)\n",
    "#     X_val_transformed = np.hstack((X_val_normal.toarray(), X_val_title.toarray()))\n",
    "    \n",
    "#     score = roc_auc_score(y_val, clf.predict_proba(X_val_transformed)[:,1])\n",
    "#     val_auc.append(score)\n",
    "#     print('[{}/{}] {}'.format((i+1)*(batch_size*2), 25000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b68cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_2.csv\")\n",
    "X_train = df['Page content']\n",
    "Y_train = df['Popularity']\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5,\n",
    "                       ngram_range=(1,3),\n",
    "                       max_features=10000,\n",
    "                       preprocessor=preprocessor,\n",
    "                       tokenizer=tokenizer_stem_nostop,\n",
    "                      ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf8296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect_title = TfidfVectorizer(\n",
    "#                        ngram_range=(1,3),\n",
    "#                        max_features=500,\n",
    "#                        preprocessor=preprocessor_title,\n",
    "#                        tokenizer=tokenizer_stem_nostop,\n",
    "#                       ).fit(X_train)\n",
    "# X_train_vectorized_title = vect_title.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0670203",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_sentiment = vectorize_sentiment(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def01d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10004)\n"
     ]
    }
   ],
   "source": [
    "# X_train_transformed = np.hstack((X_train_vectorized.toarray(), X_train_vectorized_title.toarray()))\n",
    "X_train_transformed = np.hstack((X_train_vectorized.toarray(), np.array(X_train_vectorized_sentiment)))\n",
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3579307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, X_val, y_train, y_val = train_test_split(X_train_transformed, Y_train, test_size=0.2, shuffle=True, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcc86cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7400277133167895\n",
      "0.5414102701094516\n"
     ]
    }
   ],
   "source": [
    "# clf = RandomForestClassifier(max_depth=6, n_estimators=800)\n",
    "clf = SGDClassifier(loss='log', tol=1e-3)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# print(\"acc: \",clf.score(x_train, y_train))\n",
    "# print(\"acc: \",clf.score(X_val, y_val))\n",
    "\n",
    "print(roc_auc_score(y_train, clf.predict_proba(x_train)[:,1]))\n",
    "print(roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a464fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing f1_score:  0.6125678641561741\n",
      "testing f1_score:  0.3940537989617744\n",
      "acc:  0.5355398806294086\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(x_train)\n",
    "print(\"testing f1_score: \",f1_score(y_train, y_pred))\n",
    "y_pred=clf.predict(X_val)\n",
    "print(\"testing f1_score: \",f1_score(y_val, y_pred))\n",
    "\n",
    "\n",
    "print(\"acc: \",clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffb2c6",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a121ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# stream = get_stream(path='test.csv', size=batch_size)\n",
    "# classes = np.array([0, 1])\n",
    "# pred=[]\n",
    "# index=[]\n",
    "# for i in range(11847):\n",
    "#     batch = next(stream)\n",
    "#     X_id, X_test = batch['Id'].item(), batch['Page content']\n",
    "#     output = clf.predict_proba(hashvec.transform(X_test))[:,1]\n",
    "#     index.append(X_id)\n",
    "#     pred.append(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred), len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fde044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Id': index, 'Popularity': pred}\n",
    "predict = pd.DataFrame(dict) \n",
    "predict.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c431b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be878b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
