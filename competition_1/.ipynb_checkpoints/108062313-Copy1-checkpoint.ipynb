{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab11461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4cfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9fd98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                       Page content\n",
      "0  27643  <html><head><div class=\"article-info\"><span cl...\n",
      "1  27644  <html><head><div class=\"article-info\"><span cl...\n",
      "2  27645  <html><head><div class=\"article-info\"><span cl...\n",
      "3  27646  <html><head><div class=\"article-info\"><span cl...\n",
      "4  27647  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0349d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(df['Popularity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c7903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "# print(preprocessor(df.loc[0,'Page content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75254359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "# print(tokenizer_stem(preprocessor(df.loc[0,'Page content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544c6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sam', 'laird2013', 'utcsocc', 'star', 'get', 'twitter', 'death', 'threat', 'tackl', 'one', 'direct', 'member', 'note', 'human', 'one', 'direct', 'fandom', 'nothin', 'mess', 'british', 'soccer', 'star', 'learn', 'hard', 'way', 'weekend', 'pop', 'band', 'fanboy', 'fangirl', 'brought', 'rucku', 'twitter', 'feed', 'tweet', 'fusillad', 'death', 'threat', 'follow', 'incident', 'run', 'sunday', 'chariti', 'soccer', 'match', 'see', 'also', 'gq', 'get', 'twitter', 'death', 'threat', 'one', 'direct', 'magazin', 'cover', 'gabriel', 'agbonlahor', 'time', 'lead', 'goal', 'scorer', 'english', 'premier', 'leagu', 'side', 'aston', 'villa', 'particip', 'chariti', 'match', 'honor', 'former', 'player', 'battl', 'leukemia', 'also', 'play', 'loui', 'tomlinson', 'one', 'fifth', 'british', 'irish', 'boy', 'band', 'one', 'direct', 'tomlinson', 'dribbl', 'ball', 'midway', 'sunday', 'match', 'pro', 'striker', 'agbonlahor', 'swoop', 'knock', 'away', 'tini', 'amateur', 'clean', 'play', 'sure', 'incident', 'contact', 'well', 'caus', 'tomlinson', 'collaps', 'grass', 'like', 'wound', 'deer', 'limp', 'promptli', 'vomit', 'hand', 'shown', 'tomlinson', 'agbonlahor', 'share', 'moment', 'left', 'field', 'seem', 'fine', 'pitch', 'twitter', 'howev', 'thing', 'get', 'start', 'one', 'direct', 'fan', 'base', 'explod', 'fit', 'mass', 'rage', 'threaten', 'end', 'agbonlahor', 'sort', 'unpleas', 'way', 'whit', 'exampl', 'mildli', 'nsfw', 'languag', 'whoever', 'push', 'loui', 'find', 'push', 'goddamn', 'empir', 'state', 'build', 'j', 'r', 'e', 'e', 'itsjamrey', 'septemb', 'loui', 'hurt', 'knee', 'find', 'guy', 'kill', 'http', 'co', 'proud', 'loui', 'latestof1d', 'septemb', 'gabby_10', 'obvious', 'need', 'jesu', 'fuck', 'find', 'kill', 'morgan', 'niallurbasicugh', 'septemb', 'fuck', 'gabby_10', 'could', 'u', 'touch', 'preciou', 'loui', 'kill', 'u', 'mutherfuck', 'lucho', 'garcía', 'lois7x', 'septemb', 'least', 'one', 'person', 'even', 'took', 'thing', 'step', 'threaten', 'indiscrimin', 'mass', 'murder', 'ever', 'see', 'loui', 'cri', 'bc', 'hurt', 'sad', 'kill', 'entir', 'human', 'race', 'karol', 'arri', 'nuttelarri', 'septemb', 'actual', 'latest', 'incid', 'rabid', 'teenybopp', 'attack', 'sport', 'star', 'onlin', 'perceiv', 'slight', 'demigod', 'idol', 'auto', 'tune', 'worship', 'howev', 'score', 'home', 'exampl', 'recal', 'time', 'long', 'ago', 'justin', 'bieber', 'fan', 'unleash', 'digit', 'furi', 'upon', 'nfl', 'hall', 'famer', 'follow', 'critic', 'bieber', 'alleg', 'reckless', 'drive', 'beef', 'goe', 'way', 'hockey', 'fan', 'twitter', 'flew', 'collect', 'rage', 'juli', 'photo', 'surfac', 'bieber', 'pose', 'quintessenti', 'bieber', 'like', 'fashion', 'stanley', 'cup', 'bonu', 'one', 'direct', 'direction', 'fuse', 'onlin', 'power', 'battl', 'bulli', 'one', 'direct', 'anti', 'bulli', 'campaign', 'offic', 'depot', 'od', 'togeth', 'bulli', 'video', 'youtub', 'officialofficedepot', 'harri', 'bulli', 'video', 'youtub', 'officialofficedepot', 'zayn', 'bulli', 'video', 'youtub', 'officialofficedepot', 'niall', 'bulli', 'video', 'youtub', 'officialofficedepot', 'loui', 'bulli', 'video', 'youtub', 'officialofficedepot', 'liam', 'bulli', 'video', 'youtub', 'officialofficedepot', 'imag', 'ian', 'gavan', 'getti', 'imag', 'soni', 'pictur', 'topic', 'entertain', 'music', 'one', 'direct', 'soccer', 'sport']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "print(tokenizer_stem_nostop(preprocessor(df.loc[0,'Page content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df131dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabulary]\n",
      "{'sam': 185, 'laird2013': 115, 'utcsocc': 228, 'star': 200, 'get': 80, 'twitter': 223, 'death': 38, 'threat': 212, 'tackl': 209, 'one': 156, 'direct': 43, 'member': 135, 'note': 148, 'human': 97, 'fandom': 60, 'nothin': 149, 'mess': 136, 'british': 21, 'soccer': 195, 'learn': 121, 'hard': 90, 'way': 232, 'weekend': 233, 'pop': 166, 'band': 13, 'fanboy': 59, 'fangirl': 61, 'brought': 22, 'rucku': 182, 'feed': 63, 'tweet': 222, 'fusillad': 75, 'follow': 70, 'incident': 103, 'run': 183, 'sunday': 205, 'chariti': 27, 'match': 134, 'see': 188, 'also': 4, 'gq': 85, 'magazin': 132, 'cover': 34, 'gabriel': 77, 'agbonlahor': 1, 'time': 214, 'lead': 119, 'goal': 82, 'scorer': 187, 'english': 50, 'premier': 170, 'leagu': 120, 'side': 193, 'aston': 8, 'villa': 230, 'particip': 158, 'honor': 94, 'former': 71, 'player': 165, 'battl': 15, 'leukemia': 124, 'play': 164, 'loui': 130, 'tomlinson': 217, 'fifth': 65, 'irish': 105, 'boy': 20, 'dribbl': 45, 'ball': 12, 'midway': 137, 'pro': 171, 'striker': 204, 'swoop': 208, 'knock': 114, 'away': 11, 'tini': 215, 'amateur': 5, 'clean': 28, 'sure': 206, 'contact': 32, 'well': 234, 'caus': 26, 'collaps': 30, 'grass': 86, 'like': 126, 'wound': 238, 'deer': 39, 'limp': 127, 'promptli': 172, 'vomit': 231, 'hand': 89, 'shown': 192, 'share': 191, 'moment': 139, 'left': 123, 'field': 64, 'seem': 189, 'fine': 67, 'pitch': 163, 'howev': 95, 'thing': 211, 'start': 201, 'fan': 58, 'base': 14, 'explod': 56, 'fit': 68, 'mass': 133, 'rage': 179, 'threaten': 213, 'end': 49, 'sort': 197, 'unpleas': 226, 'whit': 235, 'exampl': 55, 'mildli': 138, 'nsfw': 150, 'languag': 116, 'whoever': 236, 'push': 174, 'find': 66, 'goddamn': 83, 'empir': 48, 'state': 202, 'build': 23, 'j': 107, 'r': 176, 'e': 47, 'itsjamrey': 106, 'septemb': 190, 'hurt': 98, 'knee': 113, 'guy': 87, 'kill': 112, 'http': 96, 'co': 29, 'proud': 173, 'latestof1d': 118, 'gabby_10': 76, 'obvious': 152, 'need': 144, 'jesu': 108, 'fuck': 72, 'morgan': 140, 'niallurbasicugh': 147, 'could': 33, 'u': 224, 'touch': 220, 'preciou': 169, 'mutherfuck': 143, 'lucho': 131, 'garcía': 78, 'lois7x': 128, 'least': 122, 'person': 160, 'even': 53, 'took': 218, 'step': 203, 'indiscrimin': 104, 'murder': 141, 'ever': 54, 'cri': 35, 'bc': 16, 'sad': 184, 'entir': 52, 'race': 178, 'karol': 111, 'arri': 7, 'nuttelarri': 151, 'actual': 0, 'latest': 117, 'incid': 102, 'rabid': 177, 'teenybopp': 210, 'attack': 9, 'sport': 198, 'onlin': 157, 'perceiv': 159, 'slight': 194, 'demigod': 40, 'idol': 100, 'auto': 10, 'tune': 221, 'worship': 237, 'score': 186, 'home': 93, 'recal': 180, 'long': 129, 'ago': 2, 'justin': 110, 'bieber': 18, 'unleash': 225, 'digit': 42, 'furi': 73, 'upon': 227, 'nfl': 145, 'hall': 88, 'famer': 57, 'critic': 36, 'alleg': 3, 'reckless': 181, 'drive': 46, 'beef': 17, 'goe': 84, 'hockey': 92, 'flew': 69, 'collect': 31, 'juli': 109, 'photo': 161, 'surfac': 207, 'pose': 167, 'quintessenti': 175, 'fashion': 62, 'stanley': 199, 'cup': 37, 'bonu': 19, 'direction': 44, 'fuse': 74, 'power': 168, 'bulli': 24, 'anti': 6, 'campaign': 25, 'offic': 154, 'depot': 41, 'od': 153, 'togeth': 216, 'video': 229, 'youtub': 239, 'officialofficedepot': 155, 'harri': 91, 'zayn': 240, 'niall': 146, 'liam': 125, 'imag': 101, 'ian': 99, 'gavan': 79, 'getti': 81, 'soni': 196, 'pictur': 162, 'topic': 219, 'entertain': 51, 'music': 142}\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "count.fit([df.loc[0,'Page content']])\n",
    "# dictionary is stored in vocabulary_\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n{}'.format(BoW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a299b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(did, vid)\ttf\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t4\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t4\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t2\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t8\n",
      "  :\t:\n",
      "  (0, 216)\t1\n",
      "  (0, 217)\t4\n",
      "  (0, 218)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 220)\t1\n",
      "  (0, 221)\t1\n",
      "  (0, 222)\t1\n",
      "  (0, 223)\t5\n",
      "  (0, 224)\t2\n",
      "  (0, 225)\t1\n",
      "  (0, 226)\t1\n",
      "  (0, 227)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 229)\t6\n",
      "  (0, 230)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 232)\t3\n",
      "  (0, 233)\t1\n",
      "  (0, 234)\t1\n",
      "  (0, 235)\t1\n",
      "  (0, 236)\t1\n",
      "  (0, 237)\t1\n",
      "  (0, 238)\t1\n",
      "  (0, 239)\t6\n",
      "  (0, 240)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n",
      "(1, 241)\n"
     ]
    }
   ],
   "source": [
    "doc_bag = count.transform([df.loc[0,'Page content']])\n",
    "print('(did, vid)\\ttf')\n",
    "print(doc_bag)\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))\n",
    "print(doc_bag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e41fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bag = doc_bag.toarray()\n",
    "# print(doc_bag)\n",
    "# print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f7aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "glass: 11\n",
      "one: 10\n",
      "googl: 8\n",
      "direct: 8\n",
      "bulli: 8\n",
      "loui: 7\n",
      "video: 6\n",
      "officialofficedepot: 6\n",
      "youtub: 6\n",
      "septemb: 5\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content'].iloc[:3]\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones((1, bag_cnts.shape[0])))[0][bag_cnts.argsort()[::-1][:top]], np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5058a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "imag: 1.00\n",
      "topic: 1.00\n",
      "also: 1.00\n",
      "see: 1.00\n",
      "launch: 1.29\n",
      "u: 1.29\n",
      "get: 1.29\n",
      "howev: 1.29\n",
      "current: 1.29\n",
      "note: 1.29\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "glass: 0.6515168147554372\n",
      "googl: 0.47383041073122706\n",
      "consol: 0.351517515437782\n",
      "ouya: 0.351517515437782\n",
      "amazon: 0.351517515437782\n",
      "one: 0.3400420551415553\n",
      "game: 0.2812140123502256\n",
      "kickstart: 0.2812140123502256\n",
      "bulli: 0.2720336441132442\n",
      "direct: 0.2720336441132442\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e805059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hash words to 1024 buckets\n",
    "# hashvec = HashingVectorizer(n_features=2**10,\n",
    "#                             preprocessor=preprocessor,\n",
    "#                             tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# # no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "\n",
    "# # transform sentences to vectors of dimension 1024\n",
    "# doc_hash = hashvec.transform(doc)\n",
    "# print(doc_hash.shape)\n",
    "# print(doc_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16cdf9",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc88c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "# print(next(get_stream(path='train_2.csv', size=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e913d6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hashvec = HashingVectorizer(n_features=2**12, \n",
    "#                             preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# # loss='log' gives logistic regression\n",
    "# clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# # clf = PassiveAggressiveClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# batch_size = 1000\n",
    "# # stream = get_stream(path='train_2.csv', size=batch_size)\n",
    "# classes = np.array([0, 1])\n",
    "# train_auc, val_auc = [], []\n",
    "# # we use one batch for training and another for validation in each iteration\n",
    "# iters = int((26000+batch_size-1)/(batch_size))\n",
    "\n",
    "# stream = get_stream(path='train_2.csv', size=batch_size)\n",
    "# for i in range(iters):\n",
    "#     if(i<iters*0.7):\n",
    "#         batch = next(stream)\n",
    "#         X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "#         if X_train is None:\n",
    "#             break\n",
    "#         X_train = hashvec.transform(X_train)\n",
    "#         clf.partial_fit(X_train, y_train, classes=classes)\n",
    "#         score = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
    "#         train_auc.append(score)\n",
    "#         print('train: [{}/{}] {}'.format((i+1)*(batch_size), 26000, score))\n",
    "\n",
    "#     # validate\n",
    "#     else:\n",
    "#         batch = next(stream)\n",
    "#         X_val, y_val = batch['Page content'], batch['Popularity']\n",
    "#         score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "#         val_auc.append(score)\n",
    "#         print('valid: [{}/{}] {}'.format((i+1)*(batch_size), 26000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2603554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57997757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "# plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.xlabel('#Batches')\n",
    "# plt.ylabel('Auc')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b68cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_2.csv\")\n",
    "X_train = df['Page content']\n",
    "Y_train = df['Popularity']\n",
    "# hashvec = HashingVectorizer(n_features=2**12, \n",
    "#                             preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "#                         preprocessor=preprocessor,\n",
    "#                         tokenizer=tokenizer_stem_nostop)\n",
    "# tfidf.fit(data)\n",
    "# data = tfidf.transform(data)\n",
    "\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "#                         max_features=10000,\n",
    "#                         preprocessor=preprocessor,\n",
    "#                         tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5,\n",
    "                       ngram_range=(2,4),\n",
    "                       max_features=10000,\n",
    "                      ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# tfidf.fit(X_train)\n",
    "\n",
    "\n",
    "# clf = RandomForestClassifier(max_depth=6, n_estimators=1000, random_state=0)\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(\"acc: \",clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90383a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "579b6be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [237], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ham, spam)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ham\u001b[38;5;241m/\u001b[39mspam)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mget_key_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [237], line 14\u001b[0m, in \u001b[0;36mget_key_words\u001b[0;34m(data_X, data_y)\u001b[0m\n\u001b[1;32m     12\u001b[0m         spam\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mcount\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(ham, spam)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mham\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mspam\u001b[49m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def get_key_words(data_X, data_y):\n",
    "    num = 0\n",
    "    score = 0\n",
    "    ham=0\n",
    "    spam=0\n",
    "    for i in range(len(data_X)):\n",
    "#         if \"fuck\" in data_X[i] and \"shit\" in data_X[i]:\n",
    "        count = data_X[i].count(r'\\d')\n",
    "        if data_y[i]==1: #ham\n",
    "            ham+=count\n",
    "        else:\n",
    "            spam+=count\n",
    "    print(ham, spam)\n",
    "    print(ham/spam)\n",
    "get_key_words(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_length=X_train.str.len()\n",
    "add_digits=X_train.str.count(r'\\d')\n",
    "add_dollars=X_train.str.count(r'\\$')\n",
    "add_keyword1=X_train.str.count(\"killer\")\n",
    "add_keyword2=X_train.str.count(\"massacre\")\n",
    "add_keyword3=X_train.str.count(\"scandal\")\n",
    "add_keyword4=X_train.str.count(\"billion\")\n",
    "add_keyword4=X_train.str.count(\"fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6774f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(add_length))\n",
    "print(np.sum(add_digits))\n",
    "print(np.sum(add_dollars))\n",
    "print(np.sum(add_keyword1))\n",
    "print(np.sum(add_keyword2))\n",
    "print(np.sum(add_keyword3))\n",
    "print(np.sum(add_keyword4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03725074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):    \n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = add_feature(X_train_vectorized , [add_length, add_digits, add_dollars, add_keyword1, add_keyword2, add_keyword3, add_keyword4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3138885",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, X_val, y_train, y_val = train_test_split(X_train_transformed, Y_train, test_size=0.2, shuffle=True, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ec22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=8, n_estimators=600)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"acc: \",clf.score(x_train, y_train))\n",
    "print(\"acc: \",clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffb2c6",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2a121ec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [234], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43mget_stream\u001b[49m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m      3\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m pred\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_stream' is not defined"
     ]
    }
   ],
   "source": [
    "# batch_size = 1\n",
    "# stream = get_stream(path='test.csv', size=batch_size)\n",
    "# classes = np.array([0, 1])\n",
    "# pred=[]\n",
    "# index=[]\n",
    "# for i in range(11847):\n",
    "#     batch = next(stream)\n",
    "#     X_id, X_test = batch['Id'].item(), batch['Page content']\n",
    "#     output = clf.predict_proba(hashvec.transform(X_test))[:,1]\n",
    "#     index.append(X_id)\n",
    "#     pred.append(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "99228285",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '<html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA\\'s Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We\\'re asking for you to think about concepts and different approaches for what we\\'ve described here,\" William Gerstenmaier, NASA\\'s associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA\\'s Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they\\'d win Congress\\' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public\\'s money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA\\'s other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA\\'s plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon\\'s orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX\\'s Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [236], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:832\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    835\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    872\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    877\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:605\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:894\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '<html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA\\'s Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We\\'re asking for you to think about concepts and different approaches for what we\\'ve described here,\" William Gerstenmaier, NASA\\'s associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA\\'s Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they\\'d win Congress\\' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public\\'s money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA\\'s other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA\\'s plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon\\'s orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX\\'s Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>'"
     ]
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred), len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fde044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Id': index, 'Popularity': pred}\n",
    "predict = pd.DataFrame(dict) \n",
    "predict.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c431b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be878b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
